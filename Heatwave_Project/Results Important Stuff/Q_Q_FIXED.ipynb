{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85893305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\jarra\\Desktop\\Masters\\Heatwave_Project\")\n",
    "import pandas as pd\n",
    "import PT13_Functions_For_Masters_New_Test as HW_Func\n",
    "\n",
    "import Strictly_Functions_Project as SFP\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#RMSE \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1755f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def Simple_QQ_Regression(Q_step, Historical, Present, Hist_Dates, Pres_Date):\n",
    "    number = Q_step\n",
    "    Hist_All = Historical\n",
    "    Present = Present\n",
    "    Hist_QQ_Dates_St = Hist_Dates[0]\n",
    "    Hist_QQ_Dates_En = Hist_Dates[1]\n",
    "\n",
    "    Pres_QQ_Date_St = Pres_Date[0]\n",
    "    Pres_QQ_Date_En = Pres_Date[1]\n",
    "    #^ call in using function\n",
    "    #^ call in using function\n",
    "\n",
    "    Historical_30 = Hist_All.loc[Hist_QQ_Dates_St:Hist_QQ_Dates_En].reset_index()\n",
    "    Present_30 = Present.loc[Pres_QQ_Date_St:Pres_QQ_Date_En].reset_index()\n",
    "\n",
    "\n",
    "\n",
    "    #Select a nparange value lemgth that goes from Q0 to Q1, and produce the quantiles\n",
    "    QPRE = Present_30.quantile(np.arange(0,1+number,number)).round(4)\n",
    "    QPRE = QPRE.rename_axis('Quantile').reset_index()\n",
    "\n",
    "    QHIS = Historical_30.quantile(np.arange(0,1+number,number)).round(4)\n",
    "    QHIS = QHIS.rename_axis('Quantile').reset_index()\n",
    "\n",
    "\n",
    "    #Hist_All = Historical_All.reset_index()\n",
    "    #This above is the full range of historical data\n",
    "\n",
    "    #What we will do is append it to max and min values before combiniing with dat\n",
    "    Hist_Updated_Max = []\n",
    "    Hist_Updated_Min = []\n",
    "    Hist_Updated_Date = []\n",
    "\n",
    "\n",
    "\n",
    "    Hist_All = Hist_All.reset_index()\n",
    "    #For loop for all dates \n",
    "    for i in range(0,len(Hist_All)):\n",
    "        Hist_Updated_Date.append(Hist_All['date'].loc[i])\n",
    "\n",
    "\n",
    "        #Now get all the information from the Q-Q data for max and min for each date\n",
    "        #MAX\n",
    "\n",
    "        #If data shows a nan value set the updated value to nan\n",
    "        if (math.isnan(Hist_All['tmax'].loc[i])== True):\n",
    "            Hist_Updated_Max.append(np.NaN)\n",
    "        else:\n",
    "            #Set Temp old \n",
    "            Temp_Old = Hist_All['tmax'].loc[i]\n",
    "\n",
    "\n",
    "\n",
    "            #So now we get the closest value for the max:\n",
    "            Column = ['tmax']\n",
    "\n",
    "            #This finds the value where the Q-Hist of the tmax is the minimum it can be for the tmax value presented\n",
    "            Min_val = np.abs(QHIS[Column] - Temp_Old).min()\n",
    "\n",
    "            #This finds the quantile*10^5 or by the decimla place you use to find the tmax\n",
    "            closest_index =  QHIS[np.abs(QHIS[Column]- Temp_Old) == Min_val].stack().idxmin()\n",
    "\n",
    "            #Now this will use the index to find the Present Vlaue to updayte the historical value to using the index/quantile*10^%\n",
    "            Hist_Updated_Max.append(QPRE[Column].loc[closest_index[0]].values[0])\n",
    "\n",
    "        #Now get all the information from the Q-Q data for max and min for each date\n",
    "        #MIN\n",
    "\n",
    "        #If data shows a nan value set the updated value to nan\n",
    "        if (math.isnan(Hist_All['tmin'].loc[i])== True):\n",
    "            Hist_Updated_Min.append(np.NaN)\n",
    "        else:\n",
    "            #Set Temp old \n",
    "            Temp_Old = Hist_All['tmin'].loc[i]\n",
    "\n",
    "\n",
    "            #So now we get the closest value for the max:\n",
    "            Column = ['tmin']\n",
    "\n",
    "            #This finds the value where the Q-Hist of the tmax is the minimum it can be for the tmax value presented\n",
    "            Min_val = np.abs(QHIS[Column] - Temp_Old).min()\n",
    "\n",
    "            #This finds the quantile*10^5 or by the decimla place you use to find the tmax\n",
    "            closest_index =  QHIS[np.abs(QHIS[Column]- Temp_Old) == Min_val].stack().idxmin()\n",
    "\n",
    "            #Now this will use the index to find the Present Vlaue to updayte the historical value to using the index/quantile*10^%\n",
    "            Hist_Updated_Min.append(QPRE[Column].loc[closest_index[0]].values[0])\n",
    "    Hist_Updated_Date = pd.DataFrame(Hist_Updated_Date, columns=['date'])\n",
    "    Hist_Updated_Max = pd.DataFrame(Hist_Updated_Max, columns=['tmax'])\n",
    "    Hist_Updated_Min = pd.DataFrame(Hist_Updated_Min, columns=['tmin'])\n",
    "\n",
    "\n",
    "    #Now combine altogether\n",
    "    Hist_Updated = pd.concat([Hist_Updated_Date, Hist_Updated_Max, Hist_Updated_Min], axis = 1)\n",
    "    return(Hist_Updated)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c60f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hist_Updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bec677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "#Now putting it all together\n",
    "\n",
    "#Now lets load thedatasets in\n",
    "Hist_Max = pd.read_csv(r\"E:\\LIBRARY\\UNIVERSITY\\Masters Research\\Python\\Data\\FOR HOMOGENISATION\\Tmax_Est_1830_1875.csv\")\n",
    "Hist_Min = pd.read_csv(r\"E:\\LIBRARY\\UNIVERSITY\\Masters Research\\Python\\Data\\FOR HOMOGENISATION\\Tmin_Est_1830_1875.csv\")\n",
    "Present = pd.read_csv(r\"E:\\LIBRARY\\UNIVERSITY\\Masters Research\\Python\\Data\\FOR HOMOGENISATION\\ACORN_SAT_1880_2021.csv\")\n",
    "\n",
    "HMax_Col = Hist_Max.columns\n",
    "HMin_Col = Hist_Min.columns\n",
    "Present = Present\n",
    "length_col = len(HMax_Col)\n",
    "\n",
    "i =1\n",
    "\n",
    "#Fix the data so its only one trail we are looking at first, this is the columns of the max and min\n",
    "Hist_Max_Col = Hist_Max[['date',HMax_Col[i]]]\n",
    "Hist_Min_Col = Hist_Min[['date',HMin_Col[i]]]\n",
    "\n",
    "#We rename them to they have tmax and tmin\n",
    "Hist_Max_Col = Hist_Max_Col.rename(columns={HMax_Col[i]:'tmax'})\n",
    "Hist_Min_Col = Hist_Min_Col.rename(columns={HMin_Col[i]:'tmin'})\n",
    "\n",
    "#We add the historical datasets together\n",
    "Hist_Together = pd.concat([Hist_Max_Col['date'], Hist_Max_Col['tmax'],Hist_Min_Col['tmin']],axis = 1)\n",
    "\n",
    "Hist_Together =Hist_Together.round(4).set_index('date')\n",
    "Present = Present.set_index('date')\n",
    "\n",
    "#This is the first trial\n",
    "Hist_Corrected = Simple_QQ_Regression(0.0001, Hist_Together, Present, ['1846-01-01',\"1875-12-31\"], ['1880-01-01',\"1909-12-31\"])\n",
    "\n",
    "#This is the corrected temps\n",
    "Hist_Corrected = Hist_Corrected.rename(columns={'tmax':HMax_Col[i]})\n",
    "Hist_Corrected = Hist_Corrected.rename(columns={'tmin':HMin_Col[i]})\n",
    "\n",
    "#This is the full list\n",
    "Hist_Corrected_Dict_Full = Hist_Corrected\n",
    "\n",
    "\n",
    "\n",
    "for i in range(2,length_col):#length_col):\n",
    "    print(i)\n",
    "    #Fix the data so its only one trail we are looking at first\n",
    "    Hist_Max_Col = Hist_Max[['date',HMax_Col[i]]]\n",
    "    Hist_Min_Col = Hist_Min[['date',HMin_Col[i]]]\n",
    "\n",
    "\n",
    "    Hist_Max_Col = Hist_Max_Col.rename(columns={HMax_Col[i]:'tmax'})\n",
    "    Hist_Min_Col = Hist_Min_Col.rename(columns={HMin_Col[i]:'tmin'})\n",
    "\n",
    "    Hist_Together = pd.concat([Hist_Max_Col['date'], Hist_Max_Col['tmax'],Hist_Min_Col['tmin']],axis = 1)\n",
    "    Hist_Together =Hist_Together.round(4).set_index('date')\n",
    "\n",
    "    Hist_Corrected = Simple_QQ_Regression(0.0001, Hist_Together, Present, ['1846-01-01',\"1875-12-31\"], ['1880-01-01',\"1909-12-31\"])\n",
    "    \n",
    "    Hist_Corrected = Hist_Corrected.rename(columns={'tmax':HMax_Col[i]})\n",
    "    Hist_Corrected = Hist_Corrected.rename(columns={'tmin':HMin_Col[i]})\n",
    "    \n",
    "    Hist_Corrected_Dict_Full = pd.merge(Hist_Corrected_Dict_Full,Hist_Corrected,on = 'date')\n",
    "\n",
    "\n",
    "\n",
    "Hist_Corrected_Dict_Full.to_csv(r\"C:\\Users\\jarra\\Desktop\\Perth_1830_1875_Corrected.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28de31d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets load thedatasets in\n",
    "Hist_Max = pd.read_csv(r\"E:\\LIBRARY\\UNIVERSITY\\Masters Research\\Python\\Data\\FOR HOMOGENISATION\\Tmax_Est_1830_1875.csv\")\n",
    "Hist_Min = pd.read_csv(r\"E:\\LIBRARY\\UNIVERSITY\\Masters Research\\Python\\Data\\FOR HOMOGENISATION\\Tmin_Est_1830_1875.csv\")\n",
    "Present = pd.read_csv(r\"E:\\LIBRARY\\UNIVERSITY\\Masters Research\\Python\\Data\\FOR HOMOGENISATION\\ACORN_SAT_1880_2021.csv\")\n",
    "\n",
    "HMax_Col = Hist_Max.columns\n",
    "HMin_Col = Hist_Min.columns\n",
    "Present = Present\n",
    "length_col = len(HMin_Col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28ec4eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1008"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c3f557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
