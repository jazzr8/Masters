{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "091e9ab9",
   "metadata": {},
   "source": [
    "# DAILY EXTREME ESTIMATOR FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5256c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\jarra\\Desktop\\Masters\\Heatwave_Project\")\n",
    "import pandas as pd\n",
    "import PT13_Functions_For_Masters_New_Test as HW_Func\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#RMSE \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from datetime import datetime\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "cd57812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Temp_Estimation(Sub_Daily, Sub_Daily_Training,Daily_Extreme_Training, Trials):\n",
    "    '''\n",
    "    Parameters\n",
    "    --------------\n",
    "    Sub_Daily : DataFrame\n",
    "        This is the raw subdaily data you aim to estimate the maximum and minimum temperatures from.\n",
    "    \n",
    "    Sub_Daily_Training : DataFrame\n",
    "        A list with the date and temp as 2 columns and index going from 0,1...X. All values are subdaily so they have hours\n",
    "        associated with them also time is in 24 hour format.\n",
    "        \n",
    "    Daily_Extreme_Training : DataFrame\n",
    "        A list with the date and temp as 3 columns and index going from 0,1...X. All values are daily with max and min\n",
    "        associated with them also time is in 24 hour format.\n",
    "     \n",
    "    Trials : Integer\n",
    "        The number of trails you want to run the estimation training over.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Part 1: Split the Sub_Daily Training into individual hours ane combine\n",
    "    Sub_Max, Sub_Min, Hours_Avaliable = Sub_Daily_Splitter(Sub_Daily_Training)\n",
    "    print('DONE P1')\n",
    "    # Part 2: Concat the Maximum and Minimum Data to the subdaily data\n",
    "    Sub_Ext_Max, Sub_Ext_Min = concat_max_sub(Sub_Max, Sub_Min, Hours_Avaliable, Daily_Extreme_Training)\n",
    "    print('DONE P2')\n",
    "\n",
    "    #Now Every Single Available hour and max and min is ready to be used.\n",
    "    #Part 3: Split into each respective Month and add all together so its like Month_Hour_Mx/Mn\n",
    "    Monthly_Split_Dic = Month_Splitter(Hours_Avaliable,Sub_Ext_Max, Sub_Ext_Min)\n",
    "    print('DONE P3')\n",
    "\n",
    "    #Include 24 in the hours avalaible, this is to get it back to 0\n",
    "    Hours_Avaliable_Inc_24 = Hours_Avaliable.copy()\n",
    "    Hours_Avaliable_Inc_24.append(24)\n",
    "    \n",
    "    #PART 4 Is to fix up the Historical Data so it is closest to the every hour hour mark where data is avaliable\n",
    "    Sub_Daily = Closest_Hour(Sub_Daily, Hours_Avaliable_Inc_24)\n",
    "    print('DONE P4')\n",
    "\n",
    "    #PART 5 Is to sample by the length of the number of datapoints for that month and max or min\n",
    "    #Now I need to select 600 points and trail it 1000 times for each single thing in the dictionary and label the hour 0 as hour 0 run 1]\n",
    "    #and PRO Max Run 1\n",
    "    Sampled = Sampler_Trainer(Monthly_Split_Dic,Trials)\n",
    "    print('DONE P5')\n",
    "\n",
    "    #Part 6\n",
    "    #Now to apply the regression anaylsis onto the data I have provide\n",
    "    Linear_Analysis = Linear_Regression_Analysis(Trials, Hours_Avaliable, Sampled)\n",
    "    print('DONE P6')\n",
    "\n",
    "    #Part 7    \n",
    "    #Get the data into their respective max and min with the hours matching the regression data, look at the explabations\n",
    "    #above in Part 2 and Part 7 for more information\n",
    "    Max_Data = Max_Sub(Sub_Daily)\n",
    "    \n",
    "    Min_Data= Min_Sub(Sub_Daily)\n",
    "    print('DONE P7')\n",
    "\n",
    "    #Part 8 Temperature Estimation\n",
    "    Full_Temperature_Estimation= Tmax_Tmin_All_Data_Est(Trials, Max_Data, Min_Data, Linear_Analysis)\n",
    "    print('DONE P8')\n",
    "\n",
    "    #Part 9 The Best Temperature Estimation\n",
    "    Temperature_Estimation = Absolute_Estimation(Full_Temperature_Estimation, Trials)\n",
    "    print('DONE P9')\n",
    "\n",
    "    #Part 10. Adding all into DataFrames (not dictionaries)\n",
    "    Max, MaxCorr, Min, MinCorr= Cleansing_Data(Temperature_Estimation)\n",
    "    print('DONE P10')\n",
    "\n",
    "    \n",
    "    \n",
    "    return(Max, MaxCorr, Min, MinCorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "81eaf319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sub_Daily_Splitter(Data):\n",
    "    '''\n",
    "    Parameters\n",
    "    --------------\n",
    "    \n",
    "    Data : DataFrame\n",
    "        A list with the date and temp as 2 columns and index going from 0,1...X. All values are subdaily so they have hours\n",
    "        associated with them.\n",
    "        \n",
    "    Return\n",
    "    ------------\n",
    "    Sub_Max : Dictionary/DataFrames\n",
    "        The respective hours and the shifts to fit the regression and tmax calculation like the BOM has done is\n",
    "    Sub_Min : Dictionary/DataFrames\n",
    "        The respective hours and the shifts to fit the regression and tmin calculation like the BOM has done is\n",
    "    Hours_Avaliable : Array\n",
    "        All the hours that have at least 10 years worth of data\n",
    "        \n",
    "    '''\n",
    "    #Set datetime to date\n",
    "    Data_Col = Data.columns\n",
    "    Data = Data.set_index(Data_Col[0])\n",
    "    \n",
    "    #We need the hours in 24 hour format as a list.\n",
    "    Every_Hour = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]\n",
    "    \n",
    "    #Now to create the dictionaries necessary for the splitting.\n",
    "    Sub_Hourly_Dic = {}\n",
    "    # Hours_Avaliable is different to Every_Hour as it finds hours that have data with at least 10 years worth of data.\n",
    "    Hours_Avaliable = []\n",
    "    \n",
    "    #Begin the for loop\n",
    "    for HOUR in Every_Hour:\n",
    "              \n",
    "        #Locate all the data for that hour\n",
    "        Single_Hour_Data = pd.concat([Data[Data.index.hour==HOUR]],axis =0)\n",
    "        \n",
    "        #Now to check if the data has at least 10 years worth of Data\n",
    "        if (len(Single_Hour_Data) >= 3600):\n",
    "            #If it is then append it into the dictionary\n",
    "            \n",
    "            #reset the index to fix the datetime\n",
    "            Single_Hour_Data = Single_Hour_Data.reset_index()\n",
    "            #Make sure that datetime is still on the date we used\n",
    "            Single_Hour_Data[Data_Col[0]] = pd.to_datetime(Single_Hour_Data[Data_Col[0]]).dt.date\n",
    "            #Set Index back to date, but maybe not if we need to make it for Max and Min\n",
    "            Single_Hour_Data = Single_Hour_Data.set_index(Data_Col[0]).dropna()\n",
    "            \n",
    "            #Add to Dictionaries and columns\n",
    "            Sub_Hourly_Dic[\"Hour\" +\"_\"+ str(HOUR)] = Single_Hour_Data\n",
    "            \n",
    "            #This becomes useful in the next section where we get information relative to max and min temperatures.\n",
    "            Hours_Avaliable.append(HOUR)\n",
    "        \n",
    "        \n",
    "    #Now to split the data into the respective Max and Min dictionaries.\n",
    "\n",
    "    \n",
    "    '''\n",
    "    This is a bit of explanation about the choices I make with what horus I choose for this.\n",
    "    So in my previous versions of creating this function I had to choose the times of when I can locate\n",
    "    the Tmax and Tmin from. Now what noticed was firstly on the day (+0) the Tmax was generally found between \n",
    "    12pm+0 to 6pm+0. So this meant the likelyhood that the max was between 9am+0 to 9am+1. This actually aligns with what \n",
    "    times the Tmax is found between 9am+0 to 9am+1. This means that for this we need to shift the hour values of 0am+1 to 8am+1\n",
    "    to be used on this particular day.\n",
    "    \n",
    "    Now there is a trickier part. It is the tmin. Like the tmax, the tmin is calulcated by the BOM from 9am-1 to 9am+0. However\n",
    "    within my findings it turns out that the correlation at 9am-1 is much lower then at 9am+0, furthermore the afetrnoon of the \n",
    "    prvious day has a higher correlation to what the min will be the next day than the correlation of the day in focus.\n",
    "    This has me belive that the min for the day in focus is influenced much more highly by the temperatures of the previous day\n",
    "    then the temperature of the day in focus which I will go into further discussion later and read about on papers becasue this \n",
    "    is an interesting debate. But to estimate temperature more of this will be explained and explored.\n",
    "    \n",
    "    For now, I came to the conculsion that the tmin will be estimated by the 10am-1 to 10am+0 to account for the 9am+0 higher \n",
    "    correlation. For tmax it will be like the BOM standard 9am+0 to 9am+1\n",
    "    '''\n",
    "    \n",
    "    #Create the dictionaries for max and min\n",
    "    Sub_Max = {}\n",
    "    Sub_Min = {}\n",
    "    \n",
    "    #Now for loop with the hours we do have\n",
    "    for HOURS in Hours_Avaliable:\n",
    "        #Lets shift the hours\n",
    "        #Since we know the key was \"Hour_HOURS\"\n",
    "        #Extract the DataFrame for that specific hour\n",
    "        Hourly_Data =  Sub_Hourly_Dic.get('Hour_{}'.format(HOURS))\n",
    "        \n",
    "        #MAX\n",
    "        #Remember 0+0 to 8+1\n",
    "        if (HOURS in range(0,9)):\n",
    "            #Shift it negative one which means everything is pushed up, so tomorrows temp is now are todays hour.\n",
    "            Shift_Max = Hourly_Data.shift(-1, axis = 0).dropna()\n",
    "            #Append it to max dictionary\n",
    "            Sub_Max[\"Hour\" +\"_\"+ str(HOURS)+\"+1\"] =Shift_Max\n",
    "        else:\n",
    "            Shift_Max = Hourly_Data\n",
    "            #Append it to max dictionary\n",
    "            Sub_Max[\"Hour\" +\"_\"+ str(HOURS)+\"+0\"] = Shift_Max\n",
    "            \n",
    "            \n",
    "        #Min\n",
    "    \n",
    "        #Remember 9-1 to 8+0 : IN LINE WITH BOM STANDARDS, SHOULDNT MESS WITH IT\n",
    "\n",
    "        if (HOURS in range(10,23)):\n",
    "            #Shift it positive one which means everything is pushed down, so yesterdays temp is now are todays temp.\n",
    "            Shift_Min = Hourly_Data.shift(1, axis = 0).dropna()\n",
    "            #Append it to min dictionary\n",
    "            Sub_Min[\"Hour\" +\"_\"+ str(HOURS)+\"-1\"] =Shift_Min\n",
    "        else:\n",
    "            Shift_Min = Hourly_Data\n",
    "            #Append it to max dictionary\n",
    "            Sub_Min[\"Hour\" +\"_\"+ str(HOURS)+\"+0\"] = Shift_Min\n",
    "    \n",
    "    \n",
    "    \n",
    "    return(Sub_Max, Sub_Min, Hours_Avaliable)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b3e3189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_max_sub(Sub_Max,Sub_Min,Hours_Avaliable, DE_values):\n",
    "    '''\n",
    "    Parameters\n",
    "    --------------\n",
    "    Sub_Max : Dictionary/DataFrames\n",
    "        The respective hours and the shifts to fit the regression and tmax calculation like the BOM has done is\n",
    "    Sub_Min : Dictionary/DataFrames\n",
    "        The respective hours and the shifts to fit the regression and tmin calculation like the BOM has done is\n",
    "    Hours_Avaliable : Array\n",
    "        All the hours that have at least 10 years worth of data\n",
    "    DE_values : DataFrame\n",
    "        A list with the date and temp as 3 columns and index going from 0,1...X. All values are daily with max and min\n",
    "        associated with them also time is in 24 hour format.\n",
    "    \n",
    "    Return\n",
    "    ------------\n",
    "    Sub_Mx : Dictionary/DataFrame\n",
    "        A dictionary of many dataframes that associate the Tmax with the subdaily values of that day\n",
    "    \n",
    "    Sub_Mn : Dictionary/DataFrame\n",
    "        A dictionary of many dataframes that associate the Tmin with the subdaily values of that day\n",
    "    '''\n",
    "    DE_values_col = DE_values.columns\n",
    "    DE_values = DE_values.set_index(DE_values_col[0])\n",
    "    \n",
    "    \n",
    "    #Create the and Min dictionaries\n",
    "    Sub_Mx = {}\n",
    "    Sub_Mn = {}\n",
    "    \n",
    "    #Extract the max and min keys\n",
    "    Keys_Mx = list(Sub_Max)\n",
    "    Keys_Mn = list(Sub_Min)\n",
    "\n",
    "    #Go with Tmax\n",
    "    for i in range(len(Keys_Mx)):\n",
    "        #Extract the subdaily data for that hour\n",
    "        Mx_Sub = Sub_Max.get(Keys_Mx[i])\n",
    "        #Combine with Tmax where datetime si the joiner\n",
    "        Combined_Train_Mx = pd.merge(left = Mx_Sub, \n",
    "                                        right  =DE_values[DE_values_col[1]],\n",
    "                                        left_index=True,right_index=True  )\n",
    "        #Rename to Max\n",
    "        Combined_Train_Mx = Combined_Train_Mx.rename(columns={DE_values_col[1]:'Max'})\n",
    "        #Append to dictioanry \n",
    "        Sub_Mx[\"Hour\" +\"_\"+ str(Hours_Avaliable[i])] = Combined_Train_Mx\n",
    "        \n",
    "    #Min follow similar as Max\n",
    "    for j in range(len(Keys_Mn)):\n",
    "        #Extract the subdaily data for that hour\n",
    "        Mn_Sub = Sub_Min.get(Keys_Mn[j])\n",
    "        #Combine with Tmax where datetime si the joiner\n",
    "        Combined_Train_Mn = pd.merge(left = Mn_Sub, \n",
    "                                        right  =DE_values[DE_values_col[2]],\n",
    "                                        left_index=True,right_index=True  )\n",
    "        Combined_Train_Mn = Combined_Train_Mn.rename(columns={DE_values_col[2]:'Min'})\n",
    "        #Append to dictioanry \n",
    "        Sub_Mn[\"Hour\" +\"_\"+ str(Hours_Avaliable[j])] = Combined_Train_Mn\n",
    "        \n",
    "        \n",
    "    return(Sub_Mx,Sub_Mn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b53bd9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that splits it into each month\n",
    "def Month_Splitter(Hours_Avaliable,Sub_Ext_Max, Sub_Ext_Min):\n",
    "    '''\n",
    "    Parameters\n",
    "    --------------\n",
    "    Hours_Avaliable : Array\n",
    "        All the hours that have at least 10 years worth of data\n",
    "    Sub_Mx : Dictionary/DataFrame\n",
    "        A dictionary of many dataframes that associate the Tmax with the subdaily values of that day\n",
    "    Sub_Mn : Dictionary/DataFrame\n",
    "        A dictionary of many dataframes that associate the Tmin with the subdaily values of that day\n",
    "        \n",
    "    Return\n",
    "    ------------\n",
    "    Monthly_Split_Dic : Dictionary/DataFrame\n",
    "        A dictionary that has the data splkit into month and hours\n",
    "    \n",
    "    '''\n",
    "    #Lets get all the monthly arrays sorted   \n",
    "    Month_Number = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "    Month_Name = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "    \n",
    "    #Now lets create a monthly dictionary that the subdaily and its associated tmax/tmin can go in\n",
    "    Monthly_Split_Dic = {}\n",
    "    \n",
    "    #Lets begin the loop to extract the subdaily find the hours and append it all together\n",
    "    for i in Hours_Avaliable:\n",
    "        #Extract Max and Min DataFrames, as of the other function we know what the key is for the dictionary\n",
    "        Max_Data = Sub_Ext_Max.get('Hour_{}'.format(i))\n",
    "        Min_Data = Sub_Ext_Min.get('Hour_{}'.format(i))\n",
    "        \n",
    "        #Extract the Month Number and extract the data for that month\n",
    "        for q in range(len(Month_Number)):\n",
    "            #Get the data for the month only\n",
    "            Month_Max_Data = pd.concat([Max_Data[Max_Data.index.month==Month_Number[q]],], axis = 0)\n",
    "            Month_Min_Data = pd.concat([Min_Data[Min_Data.index.month==Month_Number[q]],], axis = 0)\n",
    "            #Add to Dictionary\n",
    "            Monthly_Split_Dic[Month_Name[q] +\"_\"+ str(i) + \"_\"+\"Mx\"] = Month_Max_Data\n",
    "            Monthly_Split_Dic[Month_Name[q] +\"_\"+ str(i) +\"_\"+ \"Mn\"] = Month_Min_Data\n",
    "  \n",
    "\n",
    "    return(Monthly_Split_Dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "34afcdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Closest_Hour(Data, hours): \n",
    "    '''\n",
    "    Parameters\n",
    "    -------------\n",
    "    Data: DataFrame\n",
    "        The sub_daily data we are aiming to estimate the Tmax and Tmin from wtih two columns, one with datetime and the \n",
    "        other with DataFrame\n",
    "    \n",
    "    hours:\n",
    "    The hours that are avaliable to use to get the data from this Data dataset as close as possible to the trained hours\n",
    "    as some hours may not be able to be used.\n",
    "    \n",
    "    Returns\n",
    "    -----------------\n",
    "    Dataset : DataFrame\n",
    "        A dataset that has the closest hour to one of the avalaible hours in the dataset.\n",
    "        \n",
    "    '''\n",
    "    #Get a new array\n",
    "    closest_hour= []\n",
    "    \n",
    "    #We want to match to the hour closest to the 3\n",
    "\n",
    "    for i in range(len(Data)):\n",
    "        #Extract the single day\n",
    "        Individual_Day = Data.loc[i]\n",
    "    \n",
    "        #Extract hour\n",
    "        Individual_Hour = Individual_Day['date'].hour\n",
    "        \n",
    "        #Take the closest hour\n",
    "        Closest_Ind_Hour = take_closest(hours, Individual_Hour)\n",
    "        \n",
    "        #If closest hour is 24, make sure it takes the closest hour on either side with the 23 and lower being favoured\n",
    "        if (Closest_Ind_Hour == 24):\n",
    "            Left_Check= abs(24 - hours[len(hours)-2])\n",
    "            Right_Check = abs(hours[0]-0)\n",
    "            \n",
    "            \n",
    "            if Left_Check > Right_Check:\n",
    "                Closest_Ind_Hour = hours[0]\n",
    "            else:\n",
    "                Closest_Ind_Hour = hours[len(hours)-2]\n",
    "            \n",
    "        \n",
    "        #Append the closest hour \n",
    "        closest_hour.append(Closest_Ind_Hour)\n",
    "    \n",
    "    #Add it as a series then combine to make it a dataframe\n",
    "    CL = pd.Series(closest_hour, name = 'Closest Hour')\n",
    "    Dataset = pd.merge(left = Data,right  =CL,left_index=True,right_index=True  )\n",
    "    return(Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "35ef042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_closest(myList, myNumber): \n",
    "    \"\"\"\n",
    "    Parameter\n",
    "    --------------\n",
    "    \n",
    "    Assumes myList is sorted. Returns closest value to myNumber.\n",
    "\n",
    "    If two numbers are equally close, return the smallest number.\n",
    "    \n",
    "    myList: \n",
    "        The values that the data can be closest to\n",
    "    \n",
    "    myNumber:\n",
    "        The raw value that will then be converted to the Closest Hour\n",
    "        \n",
    "    Returns\n",
    "    ---------------\n",
    "    after/before : Integer\n",
    "        Value that the hour can be closest to\n",
    "    \"\"\"\n",
    "    pos = bisect_left(myList, myNumber)\n",
    "    if pos == 0:\n",
    "        return myList[0]\n",
    "    if pos == len(myList):\n",
    "        return myList[-1]\n",
    "    before = myList[pos - 1]\n",
    "    after = myList[pos]\n",
    "    if after - myNumber < myNumber - before:\n",
    "        return after\n",
    "    else:\n",
    "        return before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a5b0668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sampler_Trainer(Data,Trials):\n",
    "    '''\n",
    "    Parameters\n",
    "    -------------\n",
    "    Data : DataFrame/Dictionary\n",
    "        A dictionary that has the data splkit into month and hours\n",
    "        \n",
    "    Trials : Integer\n",
    "        The number of trails you want to run the estimation training over.\n",
    "\n",
    "    Returns\n",
    "    -----------\n",
    "    Samples : DataFrame/Dictionary\n",
    "        Using the observations and the training data we can have created a dictionary of DataFrames\n",
    "        that have trialed that have been sampled by the lenght of the data avalaible for that month.\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Now I need to select random samples from the length of the data for each month, hour and mx or mn\n",
    "    #then do this 1000 times and label them in the columns Hour 0 as Hour 0 Run 1 and Max as Max Run 1.\n",
    "    \n",
    "    \n",
    "    #Create the dictionary that all the data will be inputed to\n",
    "    Samples = {}\n",
    "    \n",
    "    #Get the entire key column\n",
    "    Keys = list(Data)\n",
    "    \n",
    "    #Now extract the DataFrame from the dictionary for the Key\n",
    "    for keys_used in Keys:\n",
    "        #Extract and drop NaNs\n",
    "        Ind_DF = Data.get(keys_used).dropna()\n",
    "        #Now sample by the length fo the DataFrame and this is done for the first run only \n",
    "        Run1_Data = Ind_DF.sample(n=int(len(Ind_DF)),replace=True)\n",
    "        #Drop the date column with Index is 0 to Samples-1\n",
    "        Run1_Data = Run1_Data.reset_index(drop = True)\n",
    "\n",
    "        #Get the columns names\n",
    "        Col = Run1_Data.columns\n",
    "        \n",
    "        #Now change column name to make it run 1 etc\n",
    "        Run1_Data= Run1_Data.rename(columns={Col[0]:Col[0] + ' ' +  'Run 1'})\n",
    "        Run_Data= Run1_Data.rename(columns={Col[1]:Col[1] + ' ' +  'Run 1'})\n",
    "        \n",
    "        #Now develope the for loop but the trials is based off the lenght of Data\n",
    "        for rns in range(2,Trials+1):\n",
    "            #This is the now the random sampling for 1000 different samples of 600 \n",
    "            Individual_Run = Ind_DF.sample(n=int(len(Ind_DF)),replace=True)\n",
    "            #Drop the date column\n",
    "            Individual_Run = Individual_Run.reset_index(drop = True)\n",
    "            \n",
    "            #Get the columns names\n",
    "            Col = Individual_Run.columns\n",
    "        \n",
    "            #Now change column name to make it run 1 etc\n",
    "            Individual_Run= Individual_Run.rename(columns={Col[0]:Col[0] + ' ' +  'Run {}'.format(rns)})\n",
    "            Individual_Run= Individual_Run.rename(columns={Col[1]:Col[1] + ' ' +  'Run {}'.format(rns)})\n",
    "        \n",
    "            #Concate with RUNS\n",
    "            Run_Data = pd.concat([Run_Data, Individual_Run],axis=1)\n",
    "            \n",
    "        #Now add this to a new dictionary\n",
    "        Samples[keys_used + \"_\" + \"Samp\"] = Run_Data\n",
    "        \n",
    "    return(Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "16abb3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_Regression_Analysis(Trials, hours, Data):\n",
    "    '''\n",
    "    Parameters\n",
    "    --------------\n",
    "    Trials : Integer\n",
    "        The number of trails you want to run the estimation training over.\n",
    "        \n",
    "    hours : array\n",
    "    \n",
    "    Data : DataFrame/Dictionary\n",
    "        Using the observations and the training data we can have created a dictionary of DataFrames\n",
    "        that have trialed that have been sampled by the lenght of the data avalaible for that month.\n",
    "    \n",
    "    \n",
    "    Data : \n",
    "    Returns\n",
    "    --------------\n",
    "    '''\n",
    "    \n",
    "    #Create dictionaries\n",
    "    Regressed_Trial = {}\n",
    "    \n",
    "    #Define the month names\n",
    "    Month_Name = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "\n",
    "    #Being the for loop by extracting the month name\n",
    "    for month_num in range(0,12):\n",
    "        #Extract the month name_\n",
    "        Month_Str =  Month_Name[month_num]\n",
    "        #This is useful in the key as it is Aug_9_Mn_Samp where month_hour_mx/mn_samp\n",
    "        \n",
    "        #Now using the trials lets extract the trials within the particular dictionary\n",
    "        for trial_number in range(1,Trials+1):\n",
    "            #Now this is all the arrays that will be appended to at the end that include the linear\n",
    "            #regression line components, A and B and the Correlation by the Pearsonr\n",
    "            AMx_Total = []\n",
    "            BMx_Total = []\n",
    "            CORRMx_Total = []\n",
    "            Time = []\n",
    "            AMn_Total = []\n",
    "            BMn_Total = []\n",
    "            CORRMn_Total = []\n",
    "            \n",
    "            #Now for loop to extract the data and get the regression\n",
    "            for i in hours:\n",
    "                #---MAX---#\n",
    "                #Extract the maximum data\n",
    "                Mxt = Data.get('{}_{}_Mx_Samp'.format(Month_Str,i))\n",
    "                #Get the linear formula and the correlation of the data\n",
    "                AMx, BMx, corrMx = linear_regression_polyfit(Mxt['temp Run {}'.format(trial_number)],Mxt['Max Run {}'.format(trial_number)])\n",
    "                #Append it all\n",
    "                AMx_Total.append(AMx)\n",
    "                BMx_Total.append(BMx)\n",
    "                CORRMx_Total.append(corrMx)\n",
    "                #Repeat for min\n",
    "                #---MIN---#\n",
    "                Mnt = Data.get('{}_{}_Mn_Samp'.format(Month_Str,i))\n",
    "                AMn, BMn, corrMn = linear_regression_polyfit(Mnt['temp Run {}'.format(trial_number)],Mnt['Min Run {}'.format(trial_number)])\n",
    "                Time.append(int(i)) \n",
    "                AMn_Total.append(AMn)\n",
    "                BMn_Total.append(BMn)\n",
    "                CORRMn_Total.append(corrMn)\n",
    "\n",
    "            #Add it all into a dataframe\n",
    "            Time = pd.Series(Time,name = 'Hours')\n",
    "            \n",
    "            AMX = pd.Series(AMx_Total,name = 'A')\n",
    "            BMX = pd.Series(BMx_Total,name = 'B')\n",
    "            corrMX = pd.Series(CORRMx_Total,name = 'Correlation')\n",
    "            ItemsMX = pd.concat([Time,AMX,BMX,corrMX],axis = 1)\n",
    "            \n",
    "            AMN = pd.Series(AMn_Total,name = 'A')\n",
    "            BMN = pd.Series(BMn_Total,name = 'B')\n",
    "            corrMN = pd.Series(CORRMn_Total,name = 'Correlation')\n",
    "            ItemsMN = pd.concat([Time,AMN,BMN,corrMN],axis = 1)\n",
    "            \n",
    "            Regressed_Trial[\"{}\".format(Month_Str) + \"_\" + 'Trial'+ \"_\" + str(trial_number) + \"_\" + \"Mx\"] = ItemsMX\n",
    "            Regressed_Trial[\"{}\".format(Month_Str) + \"_\" + 'Trial'+ \"_\" + str(trial_number) + \"_\" + \"Mn\"] = ItemsMN\n",
    "    return(Regressed_Trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b69016a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now develop the linear regression equation\n",
    "def linear_regression_polyfit(x,y):\n",
    "    #Find the linear Relationship\n",
    "    A, B = np.polyfit(x, y, 1)\n",
    "    #Find the correlation                  \n",
    "    corr, _ = spearmanr(x, y)\n",
    "    return(A,B,corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4bddd594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Max_Sub(Data):\n",
    "    Sub_Daily_Data = Data.copy()\n",
    "    #Get the estimation sorted\n",
    "    # Shift hours 0 to 8 to the previous day's hours for maximum regression of 9am+0 to 8am+1\n",
    "    Sub_Daily_Data['date'] = pd.to_datetime(Sub_Daily_Data['date'])\n",
    "    Sub_Daily_Data.loc[Sub_Daily_Data['date'].dt.hour < 9, 'date'] = Sub_Daily_Data['date'] - pd.offsets.Day(1)\n",
    "    Sub_Daily_Data['date'] = Sub_Daily_Data['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return(Sub_Daily_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dac0bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Min_Sub(Data):\n",
    "    Sub_Daily_Data = Data.copy()\n",
    "    # Shift hours 10 to 23 to the tomorrows day's hours for minimum regression of 10am-1 to 9am+0\n",
    "    Sub_Daily_Data['date'] = pd.to_datetime(Sub_Daily_Data['date'])\n",
    "    Sub_Daily_Data.loc[Sub_Daily_Data['date'].dt.hour > 9, 'date'] = Sub_Daily_Data['date'] + pd.offsets.Day(1)\n",
    "    Sub_Daily_Data['date'] = Sub_Daily_Data['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "    return(Sub_Daily_Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "59a58a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first part of the estimation is to estimate for all times\n",
    "#The estimation matrix should be:\n",
    "#index date est_hour Tmax Tmin Corr_Max Corr_Min\n",
    "\n",
    "#We will work on the estimations for tmax and tmin individually\n",
    "def Tmax_Tmin_All_Data_Est(Trials, Historical_Max, Historical_Min, Linear):\n",
    "    '''\n",
    "    Parameters\n",
    "    --------------\n",
    "    Trials : Integer\n",
    "        Number of trials that have been used in this estimation\n",
    "    \n",
    "    Historical : DataFrame\n",
    "        The dataset we will estimate the tmax and tmin temperatures from this already should be in a good format\n",
    "    \n",
    "    Linear : Dictionary/DataFrame\n",
    "        The dictionary with all the linear regressed data for each trial ready to be applied onto the \n",
    "    \n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Now we begin with the final dictionary\n",
    "    #Write a all data didctionary\n",
    "    \n",
    "    All_Data_Est = {}\n",
    "    \n",
    "    #Columns for data \n",
    "    Historical_Max_Col = Historical_Max.columns\n",
    "    Historical_Min_Col = Historical_Min.columns\n",
    "    \n",
    "    #Lets begin with the for loop for the trials of the linear\n",
    "    for T in range(1,Trials+1):\n",
    "        print(T)\n",
    "        #Set all the arrays for the information to be added into it\n",
    "        Est_Max = []\n",
    "        Est_Min = []\n",
    "        Max_Corr = []\n",
    "        Min_Corr = []\n",
    "        \n",
    "        \n",
    "        #Now lets begin with Mx\n",
    "        for indexed in range(len(Historical_Max)):\n",
    "            #Extract the initial data\n",
    "            Day_Data_Max = Historical_Max.loc[indexed]\n",
    "            \n",
    "            \n",
    "            \n",
    "            #Extract these values : closest hour, month, temp\n",
    "            Month_V_Max = datetime.strptime(Day_Data_Max[Historical_Max_Col[0]], '%Y-%m-%d %H:%M:%S').month\n",
    "            Hour_Max = Day_Data_Max[Historical_Max_Col[2]]\n",
    "            Temperature_Max = Day_Data_Max[Historical_Max_Col[1]]\n",
    "            \n",
    "            #Now using another function we can sift through the trial and month to find the estimation,\n",
    "            Mx_Temp, Corr_Mx = The_Estimator(Month_V_Max, Hour_Max, Temperature_Max, Linear, T, True)\n",
    "        \n",
    "            Est_Max.append(Mx_Temp)\n",
    "            Max_Corr.append(Corr_Mx)\n",
    "        \n",
    "        \n",
    "        #Add the data to the dates again\n",
    "        Est_Max = pd.Series(Est_Max,name = 'Max Temp Estimation')\n",
    "        Max_Corr = pd.Series(Max_Corr,name = 'Correlation Max T')\n",
    "        \n",
    "        Dataset_Max = pd.concat([Historical_Max, Est_Max, Max_Corr],axis=1)\n",
    "        \n",
    "        #Now lets begin with Mx\n",
    "        for indexed in range(len(Historical_Min)):\n",
    "            #Extract the initial data\n",
    "            Day_Data_Min = Historical_Min.loc[indexed]\n",
    "            \n",
    "            #Extract these values : closest hour, month, temp\n",
    "            Month_V_Min = datetime.strptime(Day_Data_Min[Historical_Min_Col[0]], '%Y-%m-%d %H:%M:%S').month\n",
    "            Hour_Min = Day_Data_Min[Historical_Min_Col[2]]\n",
    "            Temperature_Min = Day_Data_Min[Historical_Min_Col[1]]\n",
    "            \n",
    "            #Now using another function we can sift through the trial and month to find the estimation,\n",
    "            Mn_Temp, Corr_Mn = The_Estimator(Month_V_Min, Hour_Min, Temperature_Min, Linear, T, False)\n",
    "        \n",
    "            Est_Min.append(Mn_Temp)\n",
    "            Min_Corr.append(Corr_Mn)\n",
    "        \n",
    "        \n",
    "        #Add the data to the dates again\n",
    "        Est_Min = pd.Series(Est_Min,name = 'Min Temp Estimation')\n",
    "        Min_Corr = pd.Series(Min_Corr,name = 'Correlation Min T')\n",
    "        \n",
    "        Dataset_Min = pd.concat([Historical_Min, Est_Min, Min_Corr],axis=1)\n",
    "        \n",
    "        \n",
    "        #Add to a Trial Dictionary\n",
    "        All_Data_Est['Trial' + '_' + str(T) + \"_Mx\"] = Dataset_Max\n",
    "        All_Data_Est['Trial' + '_' + str(T) + \"_Mn\"] = Dataset_Min\n",
    "            \n",
    "\n",
    "        \n",
    "    return(All_Data_Est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1f09b2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def The_Estimator(MONTH, Hour, Temp, DATA_4_EST, Trial_Number, Max):\n",
    "    Month_Name = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "    \n",
    "    \n",
    "    if (Max == True):\n",
    "        #Estimate the Max Temp\n",
    "        #Extract data from the linear regression dictionary\n",
    "        Info = DATA_4_EST.get('{}_Trial_{}_Mx'.format(Month_Name[MONTH-1],Trial_Number))\n",
    "        Info = Info.set_index('Hours')\n",
    "        Info = Info.loc[int(Hour)]\n",
    "        #Estimate the Max based off this information\n",
    "        Est_Max = Info['A']*Temp + (Info['B'])\n",
    "        #Find the Corr for this day and hour\n",
    "        Corr_Max =  Info['Correlation']\n",
    "        return(Est_Max,Corr_Max)\n",
    "    else:\n",
    "        #Estimate the Min Temp\n",
    "        #Extract data from the linear regression dictionary\n",
    "        Info = DATA_4_EST.get('{}_Trial_{}_Mn'.format(Month_Name[MONTH-1],Trial_Number))\n",
    "        Info = Info.set_index('Hours')\n",
    "        Info = Info.loc[int(Hour)]        #Estimate the Max based off this information\n",
    "        Est_Min = Info['A']*Temp + (Info['B'])\n",
    "        #Find the Corr for this day and hour\n",
    "        Corr_Min =  Info[\"Correlation\"]\n",
    "        return(Est_Min,Corr_Min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2c7b4038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make sure each max and min day does get chosen, we will have to estimate both indiviudally before they are combined for\n",
    "#for that day\n",
    "def Absolute_Estimation(Estimated_Data, Trials):\n",
    "    #We need a new dictionary for the finalised estimation\n",
    "    Est_Daily_Extremes = {}\n",
    "    \n",
    "\n",
    "    #Lets begin by using a for loop that extracts that Trail number and the indivudal max and min estimations\n",
    "    for T in range(1,Trials+1):\n",
    "        print(T)\n",
    "\n",
    "        #Extract the data\n",
    "        Max_Data = Estimated_Data.get('Trial_{}_Mx'.format(T))\n",
    "        Min_Data = Estimated_Data.get('Trial_{}_Mn'.format(T))\n",
    "    \n",
    "        #Lets extract the columns as well, this will be useful.\n",
    "        #Get Columns\n",
    "        Max_C = Max_Data.columns\n",
    "        Min_C = Min_Data.columns\n",
    "    \n",
    "        #Make the data datetime \n",
    "        #Convert date to datetime\n",
    "        Max_Data[Max_C[0]] = pd.to_datetime(Max_Data[Max_C[0]])\n",
    "        Min_Data[Min_C[0]] = pd.to_datetime(Min_Data[Min_C[0]])\n",
    "    \n",
    "    \n",
    "        #Delete the hour out of the date\n",
    "        Max_Data[Max_C[0]] = Max_Data[Max_C[0]].dt.date \n",
    "        Min_Data[Min_C[0]] = Min_Data[Min_C[0]].dt.date \n",
    "        \n",
    "    \n",
    "    \n",
    "        #Now we want to see only the individual dates only\n",
    "        Unique_dates_Max = Max_Data[[Max_C[0]]].drop_duplicates()\n",
    "        Unique_dates_Max =  Unique_dates_Max.reset_index(drop = True)\n",
    "        Unique_dates_Min = Min_Data[[Min_C[0]]].drop_duplicates()\n",
    "        Unique_dates_Min =  Unique_dates_Min.reset_index(drop = True)\n",
    "        \n",
    "        #Redo datetime because for some reason when removing the hour it resets the date\n",
    "        Max_Data[Max_C[0]] = pd.to_datetime(Max_Data[Max_C[0]])\n",
    "        Min_Data[Min_C[0]] = pd.to_datetime(Min_Data[Min_C[0]])\n",
    "    \n",
    "        #Now we have the necessary data for estimated for a single day.\n",
    "        #Now define the vectors for the Max, Min, Max_Corr, and Min_Corr\n",
    "        Tmax = []\n",
    "        Corr_Max = []\n",
    "        Tmin = []\n",
    "        Corr_Min = []\n",
    "        Dates_Mx = []\n",
    "        Dates_Mn = []\n",
    "        #Now go through the max and min and choose the best value either in a simple or complex case\n",
    "        #Max\n",
    "        for i in range(len(Unique_dates_Max)):\n",
    "            #Get the individual date\n",
    "            loc_date_Mx = Max_Data.loc[Max_Data[Max_C[0]] == '{}-{}-{}'.format(Unique_dates_Max[Max_C[0]][i].year,Unique_dates_Max[Max_C[0]][i].month,Unique_dates_Max[Max_C[0]][i].day)]\n",
    "            #iT is in its length, the 1 length data is remaining with the index as the row\n",
    "            #from here we will then select either complex or simple and then go into another function.\n",
    "            Max_Est,Max_Corr = Choice_Model(loc_date_Mx, True)\n",
    "        \n",
    "            Tmax.append(Max_Est)\n",
    "            Corr_Max.append(Max_Corr)\n",
    "    \n",
    "    \n",
    "        #Min\n",
    "        for i in range(len(Unique_dates_Min)):\n",
    "            #Get the individual date\n",
    "            loc_date_Mn = Min_Data.loc[Min_Data[Min_C[0]] == '{}-{}-{}'.format(Unique_dates_Min[Min_C[0]][i].year,Unique_dates_Min[Min_C[0]][i].month,Unique_dates_Min[Min_C[0]][i].day)]\n",
    "            #iT is in its length, the 1 length data is remaining with the index as the row\n",
    "            #from here we will then select either complex or simple and then go into another function.\n",
    "            Min_Est,Min_Corr = Choice_Model(loc_date_Mn, False)\n",
    "            Tmin.append(Min_Est)\n",
    "            Corr_Min.append(Min_Corr)\n",
    "        \n",
    "        Tmax_A = pd.Series(Tmax,name = 'Max Temp Estimation')\n",
    "        Tmin_A = pd.Series(Tmin,name = 'Min Temp Estimation')\n",
    "        Corr_Max_A = pd.Series(Corr_Max,name = 'Correlation Max T')\n",
    "        Corr_Min_A = pd.Series(Corr_Min,name = 'Correlation Min T')\n",
    "        Estimated_Temp_Max = pd.concat([Unique_dates_Max, Tmax_A,Corr_Max_A],axis=1)\n",
    "        Estimated_Temp_Min = pd.concat([Unique_dates_Min, Tmin_A,Corr_Min_A],axis=1)\n",
    "        \n",
    "        #Now add it to 1 single DataFrame\n",
    "        Estimated_Merge = pd.merge(Estimated_Temp_Max, Estimated_Temp_Min, on=Max_C[0], how='outer')\n",
    "        \n",
    "        # Create a date range with missing dates\n",
    "        start_date = str(Estimated_Merge[Max_C[0]][0])\n",
    "        end_date = str(Estimated_Merge[Max_C[0]][len(Estimated_Merge)-1])\n",
    "        date_range = pd.date_range(start=start_date, end=end_date)\n",
    "        # Create a DataFrame with the missing dates\n",
    "        missing_dates_df = pd.DataFrame({Max_C[0]: date_range})\n",
    "        \n",
    "        #Now add all together so its one continue daily plot\n",
    "        Estimated_Merge[Max_C[0]] = pd.to_datetime(Estimated_Merge[Max_C[0]])\n",
    "        \n",
    "            \n",
    "        # Merge the original DataFrame with the missing dates DataFrame\n",
    "        Daily_Extremes_Est = pd.merge(missing_dates_df, Estimated_Merge, on=Max_C[0], how='outer')\n",
    "        \n",
    "        #set date as index\n",
    "        Daily_Extremes_Est = Daily_Extremes_Est.set_index(Max_C[0])\n",
    "        #Add to Dictionary\n",
    "        Est_Daily_Extremes['Trial'+ \"_\" + str(T)] = Daily_Extremes_Est\n",
    "    return(Est_Daily_Extremes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b65bc448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Choice_Model(data, Max):\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Lets work on the Max only version, this should be like in temp est v4 \n",
    "    \n",
    "    So like complex est 2 it has 3 modes, the lenth 1, 2 or more\n",
    "    \n",
    "    the first two options are the same as complex v2 but if there are 3 or more then thats when we start diverging \n",
    "    from the estimation complex variant 1\n",
    "    \n",
    "    More weight on the initial correlation and temp estimation\n",
    "    \n",
    "    '''\n",
    "\n",
    "    #Single Case: Length of 1\n",
    "    if (len(data) == 1):\n",
    "        '''2.'''\n",
    "        #This finds the values required for a length of 1 data\n",
    "        if(Max ==True):\n",
    "            data = data.reset_index(drop = True)\n",
    "            Estimated_Max = data['Max Temp Estimation'].loc[0]\n",
    "            Correlation_Max = data['Correlation Max T'].loc[0]\n",
    "            return(Estimated_Max,Correlation_Max)\n",
    "        else:\n",
    "            data = data.reset_index(drop = True)\n",
    "            Estimated_Min = data['Min Temp Estimation'].loc[0]\n",
    "            Correlation_Min = data['Correlation Min T'].loc[0]\n",
    "           \n",
    "                \n",
    "            return(Estimated_Min,Correlation_Min)\n",
    "\n",
    "    #The only 2 datavalues choices\n",
    "    elif (len(data) == 2):\n",
    "        #Begin with Max\n",
    "        if(Max == True):\n",
    "            #Gets the value of the highest correlation first\n",
    "            Highest_Correlation =  data.loc[data['Correlation Max T'] == data['Correlation Max T'].max()]\n",
    "            Highest_Correlation = Highest_Correlation.reset_index(drop = True)\n",
    "                \n",
    "            #Now lets check if maximum estimation is the highest maximum value of the day\n",
    "            Estimated_Temp = Highest_Correlation['Max Temp Estimation'].loc[0]\n",
    "            #Lets see if the highest temperature is hiher then the estimated temp\n",
    "            Highest_Actual_Temperature = data.loc[data['temp'] == data['temp'].max()]\n",
    "            Highest_Actual_Temperature = Highest_Actual_Temperature.reset_index(drop =True)\n",
    "            if(Estimated_Temp > Highest_Actual_Temperature['temp'].loc[0]):\n",
    "                #Keep estimated temp\n",
    "                return(Estimated_Temp, Highest_Correlation['Correlation Max T'].loc[0])\n",
    "            else:\n",
    "                #Choose the estimated highest actual temperature\n",
    "                return(Highest_Actual_Temperature['Max Temp Estimation'].loc[0],Highest_Actual_Temperature['Correlation Max T'].loc[0])\n",
    "\n",
    "                \n",
    "        else:\n",
    "            #Gets the value of the highesr correlation first\n",
    "            Highest_Correlation =  data.loc[data['Correlation Min T'] == data['Correlation Min T'].max()]\n",
    "            Highest_Correlation = Highest_Correlation.reset_index(drop = True)\n",
    "            #Now lets check if minimum estimation is lower then the lowest minimum value of the day\n",
    "            Estimated_Temp = Highest_Correlation['Min Temp Estimation'].loc[0]\n",
    "            #Lets see if the highest temperature is hiher then the estimated temp\n",
    "            Lowest_Actual_Temperature = data.loc[data['temp'] == data['temp'].min()]\n",
    "            Lowest_Actual_Temperature = Lowest_Actual_Temperature.reset_index(drop =True)    \n",
    "                \n",
    "            if(Estimated_Temp < Lowest_Actual_Temperature['temp'].loc[0]):\n",
    "                #Keep estimated temp\n",
    "                return(Estimated_Temp, Highest_Correlation['Correlation Min T'].loc[0])\n",
    "            else:\n",
    "                #Choose the estimated lowest actual temperature\n",
    "                return(Lowest_Actual_Temperature['Min Temp Estimation'].loc[0],Lowest_Actual_Temperature['Correlation Min T'].loc[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    else:\n",
    "        #This is for 3 or more variables\n",
    "        '''\n",
    "        So the criteria for this one is:\n",
    "        1. We begin by choosing the temperature of highest correlation known\n",
    "        2. From this value we will then check whether at least 1 observational max is above or min is below the \n",
    "        estimated value        \n",
    "        3.a If None are keep the value of the highest correlated temp\n",
    "        3.b If there are extract those values\n",
    "        4. Using 3.b check whether the correlation of any are above 0.85 then choose the highest one and keep that\n",
    "        only\n",
    "        \n",
    "        '''\n",
    "        #Decide is chosing max or min\n",
    "        if(Max == True):\n",
    "            #Gets the value of the highest correlation first\n",
    "            Highest_Correlation =  data.loc[data['Correlation Max T'] == data['Correlation Max T'].max()]\n",
    "            Highest_Correlation = Highest_Correlation.reset_index(drop = True)\n",
    "\n",
    "            #Now lets check if there are more values that are hotter observationally then this\n",
    "            Estimated_Temp = Highest_Correlation['Max Temp Estimation'].loc[0]\n",
    "\n",
    "            #Lets see if the highest temperature is hiher then the estimated temp\n",
    "            Highest_Actual_Temperature = data.loc[data['temp'] == data['temp'].max()]\n",
    "            Highest_Actual_Temperature = Highest_Actual_Temperature.reset_index(drop = True) ##\n",
    "\n",
    "            #Now do the checking function or step 2\n",
    "            if(Estimated_Temp < Highest_Actual_Temperature['temp'].loc[0]):\n",
    "                #Extract all values that have temperatures higher then this\n",
    "                Highest_Actual_Temperature = data.loc[data['temp'] >= Estimated_Temp]\n",
    "                #Drop any with a correlation of less then 0.85\n",
    "                Highest_Actual_Temperature = Highest_Actual_Temperature.loc[Highest_Actual_Temperature['Correlation Max T'] >= 0.85]\n",
    "                \n",
    "                Highest_Actual_Temperature = Highest_Actual_Temperature.reset_index(drop = True) ##\n",
    "                \n",
    "                #Check if there is more then  one varibale \n",
    "                if (len(Highest_Actual_Temperature) >= 1):\n",
    "                    #Choose the highest correlated value\n",
    "                    Highest_Correlation_Temp =  Highest_Actual_Temperature.loc[Highest_Actual_Temperature['Correlation Max T'] == Highest_Actual_Temperature['Correlation Max T'].max()]##hIGHEST CORRE TO HIGHEST ACTU\n",
    "                    Highest_Correlation_Temp = Highest_Correlation_Temp.reset_index(drop = True)\n",
    "                    return(Highest_Correlation_Temp['Max Temp Estimation'].loc[0],Highest_Correlation_Temp['Correlation Max T'].loc[0])\n",
    "                else: \n",
    "                    #Return the estimated one from before\n",
    "                    return(Estimated_Temp,Highest_Correlation['Correlation Max T'].loc[0])\n",
    "\n",
    "            else:\n",
    "                return(Estimated_Temp,Highest_Correlation['Correlation Max T'].loc[0])\n",
    "\n",
    "        else:\n",
    "            #Gets the value of the highest correlation first\n",
    "            Highest_Correlation =  data.loc[data['Correlation Min T'] == data['Correlation Min T'].max()]\n",
    "            Highest_Correlation = Highest_Correlation.reset_index(drop = True)\n",
    "\n",
    "            #Now lets check if there are more values that are hotter observationally then this\n",
    "            Estimated_Temp = Highest_Correlation['Min Temp Estimation'].loc[0]\n",
    "\n",
    "            #Lets see if the highest temperature is hiher then the estimated temp\n",
    "            Lowest_Actual_Temperature = data.loc[data['temp'] == data['temp'].min()]\n",
    "            Lowest_Actual_Temperature = Lowest_Actual_Temperature.reset_index(drop = True) ##\n",
    "\n",
    "           #Now do the checking function or step 2\n",
    "            if(Estimated_Temp > Lowest_Actual_Temperature['temp'].loc[0]):\n",
    "                #Extract all values that have temperatures higher then this\n",
    "                Lowest_Actual_Temperature = data.loc[data['temp'] >= Estimated_Temp]\n",
    "                #Drop any with a correlation of less then 0.85\n",
    "                Lowest_Actual_Temperature = Lowest_Actual_Temperature.loc[Lowest_Actual_Temperature['Correlation Min T'] >= 0.85]\n",
    "                \n",
    "                Lowest_Actual_Temperature = Lowest_Actual_Temperature.reset_index(drop = True) ##\n",
    "                \n",
    "                #Check if there is more then  one varibale \n",
    "                if (len(Lowest_Actual_Temperature) >= 1):\n",
    "                    #Choose the highest correlated value\n",
    "                    Highest_Correlation_Temp =  Lowest_Actual_Temperature.loc[Lowest_Actual_Temperature['Correlation Min T'] == Lowest_Actual_Temperature['Correlation Min T'].max()]##hIGHEST CORRE TO HIGHEST ACTU\n",
    "                    Highest_Correlation_Temp = Highest_Correlation_Temp.reset_index(drop = True)\n",
    "                    return(Highest_Correlation_Temp['Min Temp Estimation'].loc[0],Highest_Correlation_Temp['Correlation Min T'].loc[0])\n",
    "                else: \n",
    "                    #Return the estimated one from before\n",
    "                    return(Estimated_Temp,Highest_Correlation['Correlation Min T'].loc[0])\n",
    "    \n",
    "            else:\n",
    "                return(Estimated_Temp,Highest_Correlation['Correlation Min T'].loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "28176f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to DataFrame\n",
    "def Cleansing_Data(data):\n",
    "    '''\n",
    "    It is the dictionaries of all the trials and this will be just cleaned up with all relevent information to covnert\n",
    "    them into 4 dataframes\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "        #DataFrames\n",
    "    Max_DF = pd.DataFrame()\n",
    "    Min_DF = pd.DataFrame()\n",
    "    CorrMax_DF = pd.DataFrame()\n",
    "    CorrMin_DF = pd.DataFrame()\n",
    "\n",
    "    for key, df in data.items():\n",
    "        #Extract the trial number\n",
    "        trial_number = key.split('_')[1]\n",
    "\n",
    "        #Change Name of each column to Something Simple with trial Number\n",
    "        df.columns = ['Max_' + trial_number, 'MaxCorr_' + trial_number,\n",
    "                      'Min_' + trial_number, 'MinCorr_' + trial_number]\n",
    "\n",
    "        #Combine the Trials\n",
    "        Max_DF = pd.concat([Max_DF, df[df.columns[0]]], axis=1)\n",
    "        CorrMax_DF = pd.concat([CorrMax_DF, df[df.columns[1]]], axis=1)\n",
    "        Min_DF = pd.concat([Min_DF, df[df.columns[2]]], axis=1)\n",
    "        CorrMin_DF = pd.concat([CorrMin_DF, df[df.columns[3]]], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    #Add Median, Mean, Conf60, Conf90, and Range\n",
    "\n",
    "    #----------------MAX-----------------------#\n",
    "    # Calculate the median across Trials\n",
    "    Data = Max_DF\n",
    "    median_values = Data.median(axis=1).round(2)\n",
    "    median_values.name = 'Max Median'\n",
    "    # Calculate the mean across Trials\n",
    "    mean_values = Data.mean(axis=1).round(2)\n",
    "    mean_values.name = 'Max Mean'\n",
    "    # Calculate the 60% confidence interval\n",
    "    confidence_60 = Data.quantile(q=[0.2, 0.8], axis=1).T.round(2)\n",
    "    confidence_60.columns = ['Max Lower CI (60%)', 'Max Upper CI (60%)']\n",
    "    # Calculate the 90% confidence interval\n",
    "    confidence_90 = Data.quantile(q=[0.05, 0.95], axis=1).T.round(2)\n",
    "    confidence_90.columns = ['Max Lower CI (90%)', 'Max Upper CI (90%)']\n",
    "    # Calculate the full range\n",
    "    Range = Data.apply(lambda row: np.ptp(row), axis=1).round(2)\n",
    "    Range.name = 'Max Full Range'\n",
    "\n",
    "    Max_All = pd.concat([mean_values,median_values,confidence_60,confidence_90,Range,Data.round(2)],axis=1)\n",
    "\n",
    "    #----------------CORRMAX-----------------------#\n",
    "    # Calculate the median across Trials\n",
    "    Data = CorrMax_DF\n",
    "    median_values = Data.median(axis=1).round(4)\n",
    "    median_values.name = 'CorrMax Median'\n",
    "    # Calculate the mean across Trials\n",
    "    mean_values = Data.mean(axis=1).round(4)\n",
    "    mean_values.name = 'CorrMax Mean'\n",
    "    # Calculate the 60% confidence interval\n",
    "    confidence_60 = Data.quantile(q=[0.2, 0.8], axis=1).T.round(4)\n",
    "    confidence_60.columns = ['CorrMax Lower CI (60%)', 'CorrMax Upper CI (60%)']\n",
    "    # Calculate the 90% confidence interval\n",
    "    confidence_90 = Data.quantile(q=[0.05, 0.95], axis=1).T.round(4)\n",
    "    confidence_90.columns = ['CorrMax Lower CI (90%)', 'CorrMax Upper CI (90%)']\n",
    "    # Calculate the full range\n",
    "    Range = Data.apply(lambda row: np.ptp(row), axis=1).round(4)\n",
    "    Range.name = 'CorrMax Full Range'\n",
    "\n",
    "    CorrMax_All = pd.concat([mean_values,median_values,confidence_60,confidence_90,Range,Data.round(4)],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------MIN-----------------------#\n",
    "    # Calculate the median across Trials\n",
    "    Data = Min_DF\n",
    "    median_values = Data.median(axis=1).round(2)\n",
    "    median_values.name = 'Min Median'\n",
    "    # Calculate the mean across Trials\n",
    "    mean_values = Data.mean(axis=1).round(2)\n",
    "    mean_values.name = 'Min Mean'\n",
    "    # Calculate the 60% confidence interval\n",
    "    confidence_60 = Data.quantile(q=[0.2, 0.8], axis=1).T.round(2)\n",
    "    confidence_60.columns = ['Min Lower CI (60%)', 'Min Upper CI (60%)']\n",
    "    # Calculate the 90% confidence interval\n",
    "    confidence_90 = Data.quantile(q=[0.05, 0.95], axis=1).T.round(2)\n",
    "    confidence_90.columns = ['Min Lower CI (90%)', 'Min Upper CI (90%)']\n",
    "    # Calculate the full range\n",
    "    Range = Data.apply(lambda row: np.ptp(row), axis=1).round(2)\n",
    "    Range.name = 'Min Full Range'\n",
    "\n",
    "    Min_All = pd.concat([mean_values,median_values,confidence_60,confidence_90,Range,Data.round(2)],axis=1)\n",
    "\n",
    "    #----------------CORRMAX-----------------------#\n",
    "    # Calculate the median across Trials\n",
    "    Data = CorrMin_DF\n",
    "    median_values = Data.median(axis=1).round(4)\n",
    "    median_values.name = 'CorrMin Median'\n",
    "    # Calculate the mean across Trials\n",
    "    mean_values = Data.mean(axis=1).round(4)\n",
    "    mean_values.name = 'CorrMin Mean'\n",
    "    # Calculate the 60% confidence interval\n",
    "    confidence_60 = Data.quantile(q=[0.2, 0.8], axis=1).T.round(4)\n",
    "    confidence_60.columns = ['CorrMin Lower CI (60%)', 'CorrMin Upper CI (60%)']\n",
    "    # Calculate the 90% confidence interval\n",
    "    confidence_90 = Data.quantile(q=[0.05, 0.95], axis=1).T.round(4)\n",
    "    confidence_90.columns = ['CorrMin Lower CI (90%)', 'CorrMin Upper CI (90%)']\n",
    "    # Calculate the full range\n",
    "    Range = Data.apply(lambda row: np.ptp(row), axis=1).round(4)\n",
    "    Range.name = 'CorrMin Full Range'\n",
    "\n",
    "    CorrMin_All = pd.concat([mean_values,median_values,confidence_60,confidence_90,Range,Data.round(4)],axis=1)\n",
    "\n",
    "    return(Max_All, CorrMax_All, Min_All, CorrMin_All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9dd2f80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     date  temp\n",
      "0     1967-01-01 00:00:00  18.3\n",
      "1     1967-01-01 03:00:00  17.3\n",
      "2     1967-01-01 06:00:00  17.7\n",
      "3     1967-01-01 09:00:00  25.1\n",
      "4     1967-01-01 12:00:00  31.6\n",
      "...                   ...   ...\n",
      "73864 1992-04-28 09:00:00  18.0\n",
      "73865 1992-04-28 15:00:00  22.7\n",
      "73866 1992-04-29 09:00:00  19.6\n",
      "73867 1992-04-29 15:00:00  23.0\n",
      "73868 1992-04-30 09:00:00  17.5\n",
      "\n",
      "[73869 rows x 2 columns]\n",
      "           date  PRO Max  PRO Min\n",
      "0    1967-01-01     33.1     16.4\n",
      "1    1967-01-02     27.6     20.2\n",
      "2    1967-01-03     28.2     18.9\n",
      "3    1967-01-04     24.7     17.2\n",
      "4    1967-01-05     21.4     16.1\n",
      "...         ...      ...      ...\n",
      "9246 1992-04-25     29.6     15.7\n",
      "9247 1992-04-26     30.0     19.5\n",
      "9248 1992-04-27     22.4     15.7\n",
      "9249 1992-04-28     22.7     15.7\n",
      "9250 1992-04-29     24.6     16.3\n",
      "\n",
      "[9251 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1830-04-16 12:00:00</td>\n",
       "      <td>23.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1830-04-16 15:00:00</td>\n",
       "      <td>25.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1830-04-17 10:00:00</td>\n",
       "      <td>22.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1830-04-17 15:00:00</td>\n",
       "      <td>26.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1830-04-18 10:00:00</td>\n",
       "      <td>23.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26388</th>\n",
       "      <td>1875-12-29 16:00:00</td>\n",
       "      <td>33.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26389</th>\n",
       "      <td>1875-12-30 10:00:00</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26390</th>\n",
       "      <td>1875-12-30 16:00:00</td>\n",
       "      <td>31.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26391</th>\n",
       "      <td>1875-12-31 10:00:00</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26392</th>\n",
       "      <td>1875-12-31 16:00:00</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26393 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     date  temp\n",
       "0     1830-04-16 12:00:00  23.1\n",
       "1     1830-04-16 15:00:00  25.8\n",
       "2     1830-04-17 10:00:00  22.2\n",
       "3     1830-04-17 15:00:00  26.4\n",
       "4     1830-04-18 10:00:00  23.3\n",
       "...                   ...   ...\n",
       "26388 1875-12-29 16:00:00  33.9\n",
       "26389 1875-12-30 10:00:00  25.6\n",
       "26390 1875-12-30 16:00:00  31.1\n",
       "26391 1875-12-31 10:00:00  25.0\n",
       "26392 1875-12-31 16:00:00  25.6\n",
       "\n",
       "[26393 rows x 2 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Perth Regional Office Daily Extreme Dataset\n",
    "#Load PRO in\n",
    "#BOM PERTH REGIONAL OFFICE\n",
    "MaxT_PRO = pd.read_csv(r\"E:\\LIBRARY\\UNIVERSITY\\Masters Research\\Python\\Data\\IDCJAC0010_009034_1800_Data.csv\")\n",
    "MinT_PRO = pd.read_csv(r\"E:\\LIBRARY\\UNIVERSITY\\Masters Research\\Python\\Data\\IDCJAC0011_009034_1800_Data.csv\")\n",
    "\n",
    "#Clean The data\n",
    "MaxT_PRO['Datetime']= pd.to_datetime(MaxT_PRO[['Year', 'Month', 'Day']])\n",
    "MinT_PRO['Datetime']= pd.to_datetime(MinT_PRO[['Year', 'Month', 'Day']])\n",
    "\n",
    "#Delete irrelevent columns\n",
    "del MaxT_PRO['Product code']\n",
    "del MaxT_PRO['Bureau of Meteorology station number']\n",
    "del MaxT_PRO['Year']\n",
    "del MaxT_PRO['Month']\n",
    "del MaxT_PRO['Day']\n",
    "del MaxT_PRO['Days of accumulation of maximum temperature']\n",
    "del MaxT_PRO['Quality']\n",
    "del MinT_PRO['Product code']\n",
    "del MinT_PRO['Bureau of Meteorology station number']\n",
    "del MinT_PRO['Year']\n",
    "del MinT_PRO['Month']\n",
    "del MinT_PRO['Day']\n",
    "del MinT_PRO['Days of accumulation of minimum temperature']\n",
    "del MinT_PRO['Quality']\n",
    "\n",
    "#Change the column name to date\n",
    "MaxT_PRO= MaxT_PRO.rename(columns={'Datetime':'date'})\n",
    "MinT_PRO= MinT_PRO.rename(columns={'Datetime':'date'})\n",
    "\n",
    "\n",
    "#Change the column names\n",
    "MaxT_PRO= MaxT_PRO.rename(columns={'Maximum temperature (Degree C)':'PRO Max'})\n",
    "MinT_PRO= MinT_PRO.rename(columns={'Minimum temperature (Degree C)':'PRO Min'})\n",
    "\n",
    "#Now concat it\n",
    "MaxT_PRO= MaxT_PRO.set_index('date')\n",
    "MinT_PRO= MinT_PRO.set_index('date')\n",
    "\n",
    "PRO_DE = pd.merge(left = MaxT_PRO,right  =MinT_PRO,left_index=True,right_index=True  )\n",
    "PRO_DE\n",
    "\n",
    "#Perth Regional Office 1967 to 1992 sub daily dataset\n",
    "#'''\n",
    "#Adjusted 1942, 1943 to 2am DST all others fixed\n",
    "#r\"E:\\LIBRARY\\UNIVERSITY\\Masters Research\\Python\\Data\\perthregionaloffice_subdaily_1942-1992_DST.csv\"\n",
    "\n",
    "#Ajdusted all hours from the DST to match back to 3am, adding 1 hour to all times affected by Daylight Saving,\n",
    "#questions 1942 and 1943 not sure if its 3am or 4am adjusted.\n",
    "#r\"E:\\LIBRARY\\UNIVERSITY\\Masters Research\\Python\\Data\\perthregionaloffice_subdaily_1942-1992_DST_Removed.csv\"\n",
    "#'''\n",
    "\n",
    "#Using DST rmeoved otpion\n",
    "PRO_Sub = pd.read_csv(r\"E:\\LIBRARY\\UNIVERSITY\\Masters Research\\Python\\Data\\perthregionaloffice_subdaily_1942-1992.csv\")\n",
    "\n",
    "#Had to divide by 10 as it was without decimals\n",
    "PRO_Sub['date'] = pd.to_datetime(PRO_Sub['date'],dayfirst = True)\n",
    "PRO_Sub = PRO_Sub.set_index('date')\n",
    "PRO_Sub =PRO_Sub['temp']/10 \n",
    "\n",
    "\n",
    "PRO_Sub_ES6792  = PRO_Sub.loc['1967':'1992']\n",
    "PRO_DE6792 = PRO_DE.loc['1967':'1992']\n",
    "PRO_Sub_ES6792 = PRO_Sub_ES6792.reset_index()\n",
    "PRO_Sub_ES6792\n",
    "\n",
    "#Here are the datasets used for the example\n",
    "\n",
    "Sub_Daily_Training = PRO_Sub_ES6792\n",
    "Daily_Extreme_Training = PRO_DE6792.reset_index()\n",
    "Sub_Daily = PRO_Sub.reset_index()\n",
    "\n",
    "\n",
    "#Now load in and fix Perth Gardens 1830-1875\n",
    "Per_Gard = pd.read_csv(r\"E:\\LIBRARY\\UNIVERSITY\\Masters Research\\Python\\Data\\swanriver_subdaily_1830-1875.csv\")\n",
    "Per_Gard\n",
    "#Set Datetime\n",
    "Per_Gard['date'] = pd.to_datetime(Per_Gard['date'],dayfirst = True)\n",
    "## TESTING THE FUNCTIONS\n",
    "#Temp_Estimation(Sub_Daily, Sub_Daily_Training,Daily_Extreme_Training, Trials)\n",
    "\n",
    "\n",
    "\n",
    "print(Sub_Daily_Training)\n",
    "print(Daily_Extreme_Training)\n",
    "Sub_Daily\n",
    "\n",
    "Per_Gard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "40a4bc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1942-01-01 09:00:00</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1942-01-01 15:00:00</td>\n",
       "      <td>26.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1942-01-01 21:00:00</td>\n",
       "      <td>21.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1942-01-02 09:00:00</td>\n",
       "      <td>24.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1942-01-02 15:00:00</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134523</th>\n",
       "      <td>1992-04-28 09:00:00</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134524</th>\n",
       "      <td>1992-04-28 15:00:00</td>\n",
       "      <td>22.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134525</th>\n",
       "      <td>1992-04-29 09:00:00</td>\n",
       "      <td>19.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134526</th>\n",
       "      <td>1992-04-29 15:00:00</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134527</th>\n",
       "      <td>1992-04-30 09:00:00</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134528 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date  temp\n",
       "0      1942-01-01 09:00:00  23.0\n",
       "1      1942-01-01 15:00:00  26.4\n",
       "2      1942-01-01 21:00:00  21.9\n",
       "3      1942-01-02 09:00:00  24.1\n",
       "4      1942-01-02 15:00:00  25.6\n",
       "...                    ...   ...\n",
       "134523 1992-04-28 09:00:00  18.0\n",
       "134524 1992-04-28 15:00:00  22.7\n",
       "134525 1992-04-29 09:00:00  19.6\n",
       "134526 1992-04-29 15:00:00  23.0\n",
       "134527 1992-04-30 09:00:00  17.5\n",
       "\n",
       "[134528 rows x 2 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sub_Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e90b0686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE P1\n",
      "DONE P2\n",
      "DONE P3\n",
      "DONE P4\n",
      "DONE P5\n",
      "DONE P6\n",
      "DONE P7\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "DONE P8\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "DONE P9\n",
      "DONE P10\n"
     ]
    }
   ],
   "source": [
    "Max, MaxCorr, Min, MinCorr = Temp_Estimation(Per_Gard, Sub_Daily_Training,Daily_Extreme_Training, 1000)\n",
    "\n",
    "Max.to_csv(r\"C:\\Users\\jarra\\Desktop\\Tmax_Est_1830_1875.csv\")\n",
    "MaxCorr.to_csv(r\"C:\\Users\\jarra\\Desktop\\TmaxCorrelations_Est_1830_1875.csv\")\n",
    "Min.to_csv(r\"C:\\Users\\jarra\\Desktop\\Tmin_Est_1830_1875.csv\")\n",
    "MinCorr.to_csv(r\"C:\\Users\\jarra\\Desktop\\TminCorrelations_Est_1830_1875.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96184a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6174b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf096b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286bc2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74cc9fc1",
   "metadata": {},
   "source": [
    "# The Heatwave Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c9c9436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Core Function\n",
    "def Heatwave_Function_v4(Dataset,\n",
    "                         Dates_DataFrame,\n",
    "                         CDP_Matrix,\n",
    "                         Heatwave_Detail= True,\n",
    "                         Percentile = 85,\n",
    "                         window = 7,\n",
    "                         CDP_start_end_years = [1961,1990]):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    Dataset : DataFrame\n",
    "        A Tmax and Tmin Dataset that has index as numbers and not datetime.\n",
    "        #It should be in column form date_name, Tmax, Tmin\n",
    "        datetime should be in format Year-Month-Day Already\n",
    "        \n",
    "    Dates_DataFrame : DataFrame\n",
    "        This is just a DataFrame that has the dates of 366 days ready to be used where needed. \n",
    "    \n",
    "    CDP_Matrix : Array\n",
    "        If set to [] then the functions and arguements relating to the CDP are irrelevant to the function by inputs should be\n",
    "        for the function to work properly.\n",
    "    \n",
    "    Heatwave_Detail : True or False\n",
    "        If True is selected the heatwaves will be expanded into more detail.\n",
    "        \n",
    "    Percentile : Integer/Decimal\n",
    "        A number that is used for the CDP, it calculates the value where the temperature must exceed to be in \n",
    "        that x percentile\n",
    "    \n",
    "    \n",
    "    window : Integer\n",
    "        Number of days either side of the day in focus that is used to calculate the percentile value in the CDP\n",
    "    \n",
    "    CDP_start_end_years : array of 2\n",
    "        The years when the CDP should be calculated. Forms the basis of how many heatwaves we get\n",
    "    \n",
    "    RETURNS\n",
    "    -----------------\n",
    "    \n",
    "    heatwaves : DataFrame\n",
    "        The heatwave with all the relevant information.\n",
    "        \n",
    "    CDP : DataFrame\n",
    "        Calendar Day Percentile so this can be inputted in the function again and save time.\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "    '''\n",
    "    #Extract Columns\n",
    "    Column_Dataset = Dataset.columns\n",
    "    \n",
    "    #For the calendar day percentile (CDP) function this dataset needs to be expanded to dataset_exp\n",
    "    Dataset_Exp = Date_Splitter(Dataset)\n",
    "    \n",
    "    #Now calculate the Calender Day Percentiles for tmax and tmin if required.\n",
    "    if (len(CDP_Matrix) == 0):\n",
    "        #Now to calculate the CDP for Max and Min Temperatures\n",
    "        CDP_Max = Calendar_Day_Percentile(Dataset_Exp,Percentile,\n",
    "                                      Column_Dataset[1],\n",
    "                                      CDP_start_end_years[0],\n",
    "                                      CDP_start_end_years[1],\n",
    "                                      window,\n",
    "                                      Dates_DataFrame)\n",
    "    \n",
    "        CDP_Min = Calendar_Day_Percentile(Dataset_Exp,Percentile,\n",
    "                                      Column_Dataset[2],\n",
    "                                      CDP_start_end_years[0],\n",
    "                                      CDP_start_end_years[1],\n",
    "                                      window,\n",
    "                                      Dates_DataFrame)\\\n",
    "        #Concat the tmax and tmax CDPs together\n",
    "        CDP_Max_Col = CDP_Max.columns \n",
    "        CDP_Min_Col = CDP_Min.columns \n",
    "        CDP = pd.concat([CDP_Max[CDP_Max_Col[0]],CDP_Max[CDP_Max_Col[1]],CDP_Min[CDP_Min_Col[1]]],axis=1) #Change the name\n",
    "    else:\n",
    "        CDP = CDP_Matrix\n",
    "    \n",
    "    \n",
    "    # Now using all the information, generate the Excess Heat Factor Values\n",
    "    #Lets make it simpler and calculate the EHF which has the components of EHI sig and EHI acc\n",
    "    EHF_Max, EHF_Min = EXCESS_HEAT_FACTOR(Dataset, CDP)\n",
    "    \n",
    "    #Combine all the data together in 1 big dataset\n",
    "    #Make all datetime set\n",
    "    Dataset_Date =  Dataset.set_index(Column_Dataset[0])\n",
    "    #This is finding the highest and lowest year within the dataset\n",
    "    Start_end_year = [Dataset_Date['year'].min(),Dataset_Date['year'].max()]\n",
    "    \n",
    "    #Clean the Dataset_Date up a bit \n",
    "    del Dataset_Date['year'] \n",
    "    del Dataset_Date['month']\n",
    "    del Dataset_Date['day'] \n",
    "    \n",
    "    #Remane the EHF columns so its max and min categorised\n",
    "    EHF_Max_Min_Col = EHF_Max.columns\n",
    "    EHF_Max = EHF_Max.rename(columns={EHF_Max_Min_Col[1]:EHF_Max_Min_Col[1] + '{}'.format('Max')})\n",
    "    EHF_Max = EHF_Max.rename(columns={EHF_Max_Min_Col[2]:EHF_Max_Min_Col[2] + '{}'.format('Max')})\n",
    "    EHF_Max = EHF_Max.rename(columns={EHF_Max_Min_Col[3]:EHF_Max_Min_Col[3] + '{}'.format('Max')})\n",
    "    EHF_Max_Date =  EHF_Max.set_index(EHF_Max_Min_Col[0])\n",
    "    EHF_Min = EHF_Min.rename(columns={EHF_Max_Min_Col[1]:EHF_Max_Min_Col[1] + '{}'.format('Min')})\n",
    "    EHF_Min = EHF_Min.rename(columns={EHF_Max_Min_Col[2]:EHF_Max_Min_Col[2] + '{}'.format('Min')})\n",
    "    EHF_Min = EHF_Min.rename(columns={EHF_Max_Min_Col[3]:EHF_Max_Min_Col[3] + '{}'.format('Min')})\n",
    "    EHF_Min_Date =  EHF_Min.set_index(EHF_Max_Min_Col[0])\n",
    "    \n",
    "    \n",
    "    #Add all the data together and the columns should be\n",
    "    '''\n",
    "    index \\ date \\ Max \\ Min \\ Excess Heat Factor Max \\ Heat Stress Max \\ Excess Heat Max \\ Excess Heat Factor Min \\ Heat Stress Min \\ Excess Heat Min \n",
    "    '''\n",
    "    Full_Information_Vector = pd.concat([Dataset_Date, EHF_Max_Date, EHF_Min_Date],axis=1)\n",
    "    Full_Information_Vector = Full_Information_Vector.reset_index()\n",
    "    \n",
    "    #Calculate both heatwaves and warmwaves\n",
    "    Warm_Spells_Matrix, Warm_Spells_Max_Only = Warm_Spells(Full_Information_Vector)\n",
    "    \n",
    "    #Find the heatwaves with loose ends at the start of Nov and end of Mar\n",
    "    heatwaves = Heatwave_Function(Warm_Spells_Matrix)\n",
    "    \n",
    "    #Generate an extended form of the heatwave table if required.\n",
    "    if(Heatwave_Detail == True):\n",
    "        heatwaves = Heatwave_Table_Generator(heatwaves,heatwaves_data)\n",
    "\n",
    "    return(heatwaves,heatwaves_data,CDP,Full_Information_Vector)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c2007c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def Date_Splitter(Dataset):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    Data : Dataframe \n",
    "        CSV dataframe where the data is from.\n",
    "        \n",
    "    date_title : String\n",
    "        Datetime Column Name for the extraction\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dataset : DataFrame\n",
    "        DataFrame that has 3 new columns for Year Month and Day\n",
    "\n",
    "    '''\n",
    "    #Exctract all the columns, but the one we need is column 0\n",
    "    Column_Dataset = Dataset.columns\n",
    "    #Split the data into year, month and day\n",
    "    Dataset['year'] =Dataset[Column_Dataset[0]].dt.year\n",
    "    Dataset['month']=Dataset[Column_Dataset[0]].dt.month\n",
    "    Dataset['day']  =Dataset[Column_Dataset[0]].dt.day\n",
    "    return(Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b3b03ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calendar_Day_Percentile(Data,Percentile,Column_Name,start_year,end_year, window, Dates_DataFrame):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    Data : Dataframe \n",
    "        The DataFrame in the expanded date form with year, month and day done already.\n",
    "        \n",
    "    Percentile : Integer/Decimal\n",
    "        A number that is used for the CDP, it calculates the value where the temperature must exceed to be in \n",
    "        that x percentile\n",
    "        \n",
    "    Column_Name : String\n",
    "        Determines if we are working out max or min temperatures\n",
    "        \n",
    "    start_year : Integer\n",
    "        Year you want to start the CDP from\n",
    "        \n",
    "    end_year : Integer\n",
    "        Year you want to end the CDP from\n",
    "        \n",
    "    Dates_DataFrame : DataFrame\n",
    "        These are the 366 total days that the CDP function will append to so we can extract a day and month in the future\n",
    "        when caculating the Excess Heat Factor\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    CDP : DataFrame\n",
    "        Calendar Day Percentile of the entire year from the baseline and window chsoen in DataFrame format\n",
    "\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    Start and end Years for the values to use\n",
    "    Start Year will be Nov - 1911 to Mar - 1942\n",
    "    I will classify a year heatwave as the 1911 season as Nov-1911 to Mar-1912\n",
    "\n",
    "    Years to be excluded from the data:\n",
    "    1910 and 2021 as these are incomplete\n",
    "\n",
    "    In the 1880-1900\n",
    "    This will be a different\n",
    "    '''\n",
    "    #Extract Columns \n",
    "    Column_Dataset = Data.columns\n",
    "    \n",
    "    #Set Index to Date\n",
    "    Data_Extracted = Data.set_index(Column_Dataset[0])\n",
    "    \n",
    "    #Extract the Start and End Year only and since we are starting from Summer and ending \n",
    "    #Lets go from 1911 - 1 December to 1940 - November as an example\n",
    "    \n",
    "    #Extract the Summer of first year to Last month of spring of the last year\n",
    "    Data_Extracted = Data_Extracted.loc['{}-12-01'.format(start_year-1):'{}-11-30'.format(end_year)]\n",
    "    \n",
    "    #Group By month and day\n",
    "    group_days = Data_Extracted.groupby(['month','day'])\n",
    "    Daily_Data= []\n",
    "    \n",
    "    #Now using the month and daily data for each of the 366 days put them in their separate bins\n",
    "    for groups,days in group_days:\n",
    "        #Extract the specified day bin\n",
    "        Dailypre = group_days.get_group(groups).reset_index()\n",
    "        #Get the maximum values for the entire record for that calendar day\n",
    "        Values= Dailypre[Column_Name]\n",
    "        #Make it a dataframe so it is appendable\n",
    "        Values = Values.to_frame()\n",
    "        #Append that bin to that day so there will be 366 bins with  x years of data\n",
    "        Daily_Data.append(Values[Column_Name])\n",
    "            \n",
    "        \n",
    "    #Now The Daily_Data has been done, we can then apply the CDP onto the bins for a window and estimate the value for the \n",
    "    #percentile\n",
    "    CalendarDay = TnX_Rolling(window, Daily_Data, Percentile)\n",
    "    \n",
    "    #Clean the data up\n",
    "    CDP = pd.DataFrame(CalendarDay, columns = [Column_Name])\n",
    "    CDP = pd.concat([Dates_DataFrame,CDP],axis=1)\n",
    "    CDP['date'] = pd.to_datetime(CDP['date'],format=\"%d/%m/%Y\")\n",
    "\n",
    "        \n",
    "    return(CDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d36066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TnX_Rolling(Window ,Dataset, Percentile):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    Window : Integer\n",
    "        How many days before AND after that the CDP will use up\n",
    "        \n",
    "    Dataset : DataFrame\n",
    "        It is the Daily_Data dataset that will be used from 3.\n",
    "    \n",
    "    Percentile : Integer/Decimal\n",
    "        It is the percentile the temperature must reaach to be accepted\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        TnX : Series\n",
    "        Array of length 366 of the CDP values.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    #Since we are using the quantile version we start with that, same as percentile just 100 times less.\n",
    "    percent_to_quant = Percentile/100\n",
    "    \n",
    "    \n",
    "    TnX = []\n",
    "    #Ignore warnings cause we all know its a pain in the buttox\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    \n",
    "    #Lets begin with the central day so this will then be looped around and extracts each calendar day starting with 01-01\n",
    "    for central_day in range(366):\n",
    "        Temp_Storage = []\n",
    "        #The reason its 366 because it goes from 0 to 365 which is still length of 366\n",
    "        #Now to make the loop around the day in focus and append this to the central day  \n",
    "        \n",
    "        for around_days in range(0,Window+1):\n",
    "            #First make the if statement of central_day\n",
    "            if (around_days == 0):\n",
    "                #Add the data to a storage to be used\n",
    "                Temp_Storage = Dataset[central_day].to_numpy()\n",
    "            else:\n",
    "                #This is to check the windows for the other 365 days, if <0 or >365, then it extracts it from <=365 or >=0 \n",
    "                #Lets start with the addition of the window so central_day + window\n",
    "                if ((central_day + around_days) > 365):\n",
    "                    Window_Early_Year =  central_day + around_days - 366\n",
    "                    #Append this to the Temp_Storage\n",
    "                    Temp_Storage = np.concatenate((Temp_Storage, Dataset[Window_Early_Year].to_numpy()),axis =0)\n",
    "                    #Append the negative version to the Temp_Storage\n",
    "                    Temp_Storage = np.concatenate((Temp_Storage, Dataset[central_day - around_days].to_numpy()),axis =0)\n",
    "\n",
    "                elif ((central_day - around_days < 0)):\n",
    "                    Window_Late_Year =  central_day - around_days + 366\n",
    "                    #Append this to the Temp_Storage\n",
    "                    Temp_Storage = np.concatenate((Temp_Storage, Dataset[Window_Late_Year].to_numpy()),axis =0)\n",
    "                    #Append the negative version to the Temp_Storage\n",
    "                    Temp_Storage = np.concatenate((Temp_Storage, Dataset[central_day + around_days].to_numpy()),axis =0)\n",
    "                    \n",
    "                else:\n",
    "                    #If within bounds append normally\n",
    "                    Temp_Storage = np.concatenate((Temp_Storage, Dataset[central_day + around_days].to_numpy()),axis =0)\n",
    "                    Temp_Storage = np.concatenate((Temp_Storage, Dataset[central_day - around_days].to_numpy()),axis =0)\n",
    "\n",
    "        #Create a for loop that uses the YearTempData and find the percentile for that calendar based value.\n",
    "        #Now calculate the Percentile \n",
    "        Tn = np.quantile(Temp_Storage[~np.isnan(Temp_Storage)], percent_to_quant)#Have a llok properly and code it myslef and pull out ranks and find 90th percentile\n",
    "        TnX.append(Tn)\n",
    "    return(TnX) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c13e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EXCESS_HEAT_FACTOR(Data, CDP_Data):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        Dataset : DataFrame\n",
    "            A Tmax and Tmin Dataset that has index as numbers and not datetime.\n",
    "            It should be in column form date_name, Tmax, Tmin\n",
    "            datetime should be in format Year-Month-Day Already\n",
    "        \n",
    "        CDP_Data : DataFrame\n",
    "            The calendar day percentile based off a percetnile where the temperature needs to reach to be in that percentile.\n",
    "        \n",
    "        Returns \n",
    "        ----------\n",
    "        Excess_Heat_Stress_Factor_Matrix_Max : DataFrame\n",
    "            A DataFrame that includes the Excess Heat, Heat Stress and Excess Heat Factor variables for the tmax\n",
    "        \n",
    "        Excess_Heat_Stress_Factor_Matrix_Min : DataFrame\n",
    "            A DataFrame that includes the Excess Heat, Heat Stress and Excess Heat Factor variables for the tmax\n",
    "\n",
    "        \n",
    "        '''\n",
    "        #Extract the columsn of the Data and CDP\n",
    "        Data_col = Data.columns\n",
    "        CDP_col = CDP_Data.columns\n",
    "        \n",
    "        #Extract the date title\n",
    "        Data_Date = Data_col[0]\n",
    "        CDP_Date = CDP_col[0]\n",
    "        \n",
    "        #Set the index to dateData_Date\n",
    "        Data_Date_I = Data.set_index(Data_Date)\n",
    "        CDP_Date_I = CDP_Data.set_index(Data_Date)\n",
    "        \n",
    "        #Now we need to set the data with the start and end year to what is specified.\n",
    "        #Since the extended Summer begins in November and ends in March and the EHIacc needs a 30 day average prior to the \n",
    "        #i-2 so this means from the 1-11-XXXX we need to go back 33 days prior. This sets us to 29-9-XXXX.\n",
    "        #Data_Range = Data_Date_I.loc['{}-09-29'.format(start_end_years[0]):'{}-04-30'.format(start_end_years[1]+1)]\n",
    "        \n",
    "        #Now we have the necessary data to work out the EHIsig, EHIacc and EHF for both Max, Min and ?Average?\n",
    "        #Heat_Stress\n",
    "        EHIacc_Max = Heat_Stress(Data_Date_I, Data_col[1]) \n",
    "        EHIacc_Min = Heat_Stress(Data_Date_I, Data_col[2]) \n",
    "\n",
    "        #Excess Heat\n",
    "        EHIsig_Max = Excess_Heat(CDP_Date_I,CDP_col[1], Data_Date_I, Data_col[1]) \n",
    "        EHIsig_Min = Excess_Heat(CDP_Date_I,CDP_col[2], Data_Date_I, Data_col[2]) \n",
    "        Excess_Heat_Stress_Matrix_Max = pd.merge(EHIacc_Max,EHIsig_Max,how='left',on = [Data_Date])\n",
    "        Excess_Heat_Stress_Matrix_Min = pd.merge(EHIacc_Min,EHIsig_Min,how='left',on = [Data_Date])\n",
    "        \n",
    "        #Excess Heat Factor\n",
    "        EHF_Max = Excess_Heat_Factor_Calculator(Excess_Heat_Stress_Matrix_Max)\n",
    "        EHF_Min = Excess_Heat_Factor_Calculator(Excess_Heat_Stress_Matrix_Min)\n",
    "        \n",
    "        #Combine\n",
    "        Excess_Heat_Stress_Factor_Matrix_Max = pd.merge(EHF_Max,Excess_Heat_Stress_Matrix_Max,how='left',on = [Data_Date])\n",
    "        Excess_Heat_Stress_Factor_Matrix_Min = pd.merge(EHF_Min,Excess_Heat_Stress_Matrix_Min,how='left',on = [Data_Date])\n",
    "    \n",
    "        return(Excess_Heat_Stress_Factor_Matrix_Max,Excess_Heat_Stress_Factor_Matrix_Min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edcf0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Heat_Stress(Data, Max_Min_Ave_Col):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    Data : DataFrame\n",
    "        This has the datetime as the index\n",
    "    \n",
    "    Max_Min_Col : Array\n",
    "        The choose of choosing the max or min or average column to use from the dataset\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    EHIacc_vector :  DataFrame\n",
    "        The Heat Stress DataFrame\n",
    "    '''\n",
    "    #Extract the column\n",
    "    Extracted_Data = Data[Max_Min_Ave_Col]\n",
    "    \n",
    "    #Reset the index to calculate the averages\n",
    "    Extracted_Data = Extracted_Data.reset_index()\n",
    "    Extracted_Data_col = Extracted_Data.columns\n",
    "    #Necessary Columns to append\n",
    "    #A date column\n",
    "    date_Values = []\n",
    "    #EHIacc column\n",
    "    EHIacc = []\n",
    "    \n",
    "    #Do the for loop\n",
    "    for dt in np.arange(Extracted_Data.index[0]+33,len(Data)):\n",
    "        #Extract the date index\n",
    "        Date = Extracted_Data[Extracted_Data_col[0]].loc[dt]\n",
    "        \n",
    "        #3-day mean where the day in focus is i\n",
    "        #But we need a checker to make sure all values are present\n",
    "        length_3day = len(Extracted_Data[Max_Min_Ave_Col].loc[dt-2:dt].dropna())\n",
    "        if (length_3day < 3):\n",
    "            mean_3_day = np.nan\n",
    "        else:\n",
    "            mean_3_day = Extracted_Data[Max_Min_Ave_Col].loc[dt-2:dt].mean()\n",
    "            \n",
    "        #3 to 32 day mean\n",
    "        #Now a dropna of 75% of values there means we can still work out the average\n",
    "        length_30day = len(Extracted_Data[Max_Min_Ave_Col].loc[dt-32:dt-3].dropna())\n",
    "        \n",
    "        if (length_30day < 23):\n",
    "            mean_30_day = np.nan\n",
    "        else:\n",
    "            mean_30_day = Extracted_Data[Max_Min_Ave_Col].loc[dt-32:dt-3].dropna().mean()\n",
    "        #The individual heat stress value\n",
    "        Heat_Stress_Value = mean_3_day - mean_30_day\n",
    "        #Append the date and Heat Stress Value\n",
    "        date_Values.append(Date)\n",
    "        EHIacc.append(Heat_Stress_Value)\n",
    "    \n",
    "    #Name the terms and combine\n",
    "    EHIacc = pd.DataFrame(EHIacc,columns=['Heat Stress'])\n",
    "    date_Values = pd.DataFrame(date_Values,columns=[Extracted_Data_col[0]])\n",
    "    \n",
    "    EHIacc_vector = pd.concat([date_Values, EHIacc],axis=1)\n",
    "    \n",
    "    return(EHIacc_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d87684d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Excess_Heat(CDP,CDP_max_min_ave, Data, Max_Min_Ave_Col):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    CDP : DataFrame\n",
    "        The calendar day percentile based off a percetnile where the temperature needs to reach to be in that percentile.\n",
    "    \n",
    "    CDP_max_min_ave : string\n",
    "        The choose of choosing the max or min or average column to use from the CDP dataset\n",
    "     \n",
    "    Data : DataFrame\n",
    "        This has the datetime as the index\n",
    "    \n",
    "    Max_Min_Col : string\n",
    "        The choose of choosing the max or min or average column to use from the Data dataset\n",
    "    \n",
    "    Return\n",
    "    ---------\n",
    "    EHIsig_vector :  DataFrame\n",
    "        The Excess Heat DataFrame\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Reset the index to calculate the averages of the data\n",
    "    Extracted_Data = Data.reset_index()\n",
    "    Extracted_Data_col = Extracted_Data.columns\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Necessary Columns to append\n",
    "    #A date column\n",
    "    date_Values = []\n",
    "    #EHIsig column\n",
    "    EHIsig = []\n",
    "    \n",
    "    #Do the for loop\n",
    "    for dt in np.arange(Extracted_Data.index[0]+33,len(Data)):\n",
    "        \n",
    "    \n",
    "        #Extract the date index\n",
    "        Date = Extracted_Data[Extracted_Data_col[0]].loc[dt]\n",
    "        \n",
    "        #Extract the date in the CDP column, we know the year is 2020\n",
    "        CDP_day = CDP[CDP_max_min_ave].loc['2020-{}-{}'.format(Date.month,Date.day)]\n",
    "             \n",
    "        Excess_Heat_Value = Extracted_Data[Max_Min_Ave_Col].loc[dt] -  CDP_day\n",
    "                                                       \n",
    "\n",
    "        #Append the date and Heat Stress Value\n",
    "        date_Values.append(Date)\n",
    "        EHIsig.append(Excess_Heat_Value)\n",
    "    \n",
    "    #Name the terms and combine\n",
    "    EHIsig = pd.DataFrame(EHIsig,columns=['Excess Heat'])\n",
    "    date_Values = pd.DataFrame(date_Values,columns=[Extracted_Data_col[0]])\n",
    "    \n",
    "    EHIsig_vector = pd.concat([date_Values, EHIsig],axis=1)\n",
    "    \n",
    "    \n",
    "    return(EHIsig_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5da61fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Excess_Heat_Factor_Calculator(Excess_Heat_Stress_Matrix):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    Excess_Heat_Stress_Matrix : DataFrame\n",
    "        This is a DataFrame that combines the Excess Heat, Heat Stress together in one DataFrame\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    EHF_vector : DataFrame\n",
    "        This is the combination of the Excess Heat and Heat Stress as a value for each day.\n",
    "    \n",
    "    '''\n",
    "    EH_col = Excess_Heat_Stress_Matrix.columns\n",
    "    #Col 0 : Date name, Col 1: Heat Stress Col 2: Excess Heat\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Necessary Columns to append\n",
    "    #A date column\n",
    "    date_Values = []\n",
    "    #EHIsig column\n",
    "    EHF = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Make sure when there are 2 positive it remains positive, if there are two negatives it remains negative \n",
    "    #and if one pos and one neg it remains negative\n",
    "    for dt in np.arange(Excess_Heat_Stress_Matrix.index[0],len(Excess_Heat_Stress_Matrix)):\n",
    "    \n",
    "        #Extract the date index\n",
    "        Date = Excess_Heat_Stress_Matrix[EH_col[0]].loc[dt]\n",
    "        \n",
    "        #Get the Heat Stress Term\n",
    "        HS = Excess_Heat_Stress_Matrix[EH_col[1]].loc[dt]\n",
    "        \n",
    "        #Get the Excess Heat Term \n",
    "        EH = Excess_Heat_Stress_Matrix[EH_col[2]].loc[dt]\n",
    "        \n",
    "        #Multiply together\n",
    "        \n",
    "        if ((HS <0) and (EH <0)):\n",
    "            EHF_single =  -1*EH* HS #degC^2\n",
    "        else:\n",
    "            EHF_single =  EH* HS #degC^2\n",
    "\n",
    "        #Append the date and Heat Stress Value\n",
    "        date_Values.append(Date)\n",
    "        EHF.append(EHF_single)\n",
    "        \n",
    "    #Name the terms and combine\n",
    "    EHF = pd.DataFrame(EHF,columns=['Excess Heat Factor'])\n",
    "    date_Values = pd.DataFrame(date_Values,columns=[EH_col[0]])\n",
    "    \n",
    "    EHF_vector = pd.concat([date_Values, EHF],axis=1)\n",
    "    \n",
    "    \n",
    "    return(EHF_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f85cff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Warm_Spells(Data):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    Data : DataFrame\n",
    "        Calculated from the EHF's and now used in the warm spell. The columns go like:\n",
    "        index \\ date \\ Max \\ Min \\ Excess Heat Factor Max \\ Heat Stress Max \\ Excess Heat Max \\ Excess Heat Factor Min \\ Heat Stress Min \\ Excess Heat Min \n",
    "                col 0  col 1 col 2  col 3                    col 4             col 5              col 6                   col 7              col 8                    \n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    warm_spell_df : DataFrame\n",
    "        Warm and heatwaves that are calculated by using the combination of 3 days and 2 nights definition.\n",
    "        \n",
    "    warm_spell_M_O_df :\n",
    "        Warm and heatwaves that are calculated by only the tmax component.\n",
    "    '''\n",
    "    \n",
    "    #The way that my Warm Spell definition works is that there must be at least 3 Max positive EHFs within 3 days initially\n",
    "    #with at least 2 minimums that are positive within the first 3 days. From there, the EHF can be positive in the day\n",
    "    #without too much worry of the minimum. \n",
    "    \n",
    "    #Side note: what about adding the EHF for the days temperatures\n",
    "    #However the reason behind this is because the min temp is found from 9am to 9am of the day before to the day in focus\n",
    "    #therefore some min temps might not be the day in focus 0hr-9amhr and actually be in the day before which may stuff up\n",
    "    #some values. If the min and max were calcualted from 0hr to 0hr a better method that includes a more extenisve use\n",
    "    #of the min temperature can be included.\n",
    "    \n",
    "    #Lets extract the columns first\n",
    "    Data_col = Data.columns\n",
    "    #Assign Appropiate lists and values\n",
    "    Warm_Spell_List = []\n",
    "    Warm_Spell_Max_Only_List = []\n",
    "    \n",
    "    break_days = 2 \n",
    "    \n",
    "    id_count = 0\n",
    "    id_count_M_O = 0\n",
    "    \n",
    "    Max_Count = 0\n",
    "    for dt in np.arange(Data.index[0],len(Data)):\n",
    "        #We are looking for a period of at least 3 days \n",
    "        if (Max_Count >= 3):\n",
    "            #So we have a Max_Count of 3 and greater\n",
    "            #Lets continue to add to the max count if EHF > 0\n",
    "            if (Data[Data_col[3]][dt] >= 0):\n",
    "                Max_Count = Max_Count + 1\n",
    "                #Since the heatwave hasnt broken continue\n",
    "                break_days = 0\n",
    "            else:\n",
    "                break_days = break_days + 1\n",
    "                #Now if two break days in a row stop the warm spell\n",
    "                if (break_days > 1):\n",
    "                    #This stops the heatwave creates an ID \n",
    "                    id_count_M_O = id_count_M_O + 1\n",
    "                    #This is for Max only and minu to is due to the break day part\n",
    "                    Warm_Spell_M_O  = Data.loc[dt-Max_Count:dt-2]\n",
    "                    Warm_Spell_M_O['id'] = [id_count_M_O] * len(Warm_Spell_M_O)\n",
    "                    Warm_Spell_Max_Only_List.append(Warm_Spell_M_O)\n",
    "                    \n",
    "                    \n",
    "                    #Now to check if its an actual Warm_Spell with my definition\n",
    "                    Min_Checker = Data.loc[dt-Max_Count:dt-Max_Count+2]\n",
    "                    Min_Length= len(Min_Checker[Min_Checker[Data_col[6]]>=0])\n",
    "                    \n",
    "                    if (Min_Length >= 2):\n",
    "                        #This creates an ID \n",
    "                        id_count = id_count + 1\n",
    "                        #This is for the entire warm spell\n",
    "                        Warm_Spell  = Data.loc[dt-Max_Count:dt-2]\n",
    "                        Warm_Spell['id'] = [id_count] * len(Warm_Spell)\n",
    "                        Warm_Spell_List.append(Warm_Spell)\n",
    "                        Max_Count = 0\n",
    "                    else:\n",
    "                        Max_Count=0\n",
    "                else:\n",
    "                    #This will continue the hot period until break_days > 1\n",
    "                    Max_Count = Max_Count + 1\n",
    "            \n",
    "        else:\n",
    "            #This is where the Max_Count_Prior to a heatwave is and the break days goes to 0\n",
    "            #No information is added into here and Min isnt checked as the main core point of the warm spell is the Max\n",
    "            #being 3 days in a row.\n",
    "            break_days = 0\n",
    "            if (Data[Data_col[3]][dt] >= 0):\n",
    "                Max_Count = Max_Count + 1\n",
    "            else:\n",
    "                Max_Count = 0\n",
    "        #print('Max count {}'.format(Max_Count))\n",
    "        #print('heatdays {}'.format(heat_days))\n",
    "        #print('id {}'.format(id_count))\n",
    "        #print('break {}'.format(  break_days))\n",
    "        \n",
    "    #Fix it up\n",
    "        \n",
    "    warm_spell_df = pd.concat(Warm_Spell_List,axis=0)\n",
    "    warm_spell_M_O_df = pd.concat(Warm_Spell_Max_Only_List,axis=0)\n",
    "    return(warm_spell_df,warm_spell_M_O_df)\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f577545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Heatwave_Function(Data):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    Data : DataFrame\n",
    "        The warm and heatwaves DataFrame\n",
    "        date / Max / Min / Excess Heat FactorMax/Heat StressMax/Excess HeatMax/Excess Heat FactorMin/Heat StressMin/Excess HeatMin/id\n",
    "        col 0 col 1 col 2  col 3                  col 4          col 5           col 6                   col 7        col 8         col 9\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Heatwaves : DataFrames\n",
    "        The warm and heatwaves DataFrame is then reduced to Nov to Mar aka the Extended Summer Season for heatwave research.\n",
    "    '''\n",
    "    #Extract Columns\n",
    "    Data_Col = Data.columns  \n",
    "    \n",
    "    #Get dates into days months and years\n",
    "    Hot_Per = Date_Splitter(Data)\n",
    "    #it will come out with, month year and day\n",
    "    \n",
    "    #This finds the heatwaves that reside in the extended summer period defined by Novmeber to March\n",
    "    ext_sum_heatwave = Hot_Per[Hot_Per['month']>=11]\n",
    "    ext_sum_heatwave2 =  Hot_Per[Hot_Per['month']<=3]\n",
    "    \n",
    "    Extended_Summer_Season = pd.concat([ext_sum_heatwave,ext_sum_heatwave2]).sort_values(by=[Data_Col[0]], ascending=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Generate a list of ids that will be used and checked to see if they are on the bounds of Nov and March\n",
    "    #as these are o as the bounds cut off heatwaves that begin or end of Nov and Mar respectively\n",
    "    id_Max = Extended_Summer_Season['id'] \n",
    "    ids = id_Max.drop_duplicates( keep='first', inplace=False)\n",
    "\n",
    "    \n",
    "    \n",
    "    '''The checker for the left and right bounds'''\n",
    "    for i in ids:\n",
    "        #Checks November-1\n",
    "        CheckL = Extended_Summer_Season[Extended_Summer_Season['id']==i]\n",
    "        LeftCheck = CheckL[CheckL['day']==1]\n",
    "        LeftCheck = LeftCheck[LeftCheck['month']==11]\n",
    "        #Checks March-31\n",
    "        CheckR = Extended_Summer_Season[Extended_Summer_Season['id']==i]\n",
    "        RightCheck = CheckR[CheckR['day']==31]\n",
    "        RightCheck = RightCheck[RightCheck['month']==3]\n",
    "        \n",
    "      \n",
    "        #If there is a value on the ends here it add it to the heatwave list\n",
    "        if (len(LeftCheck) == 1):\n",
    "            \n",
    "            Extended_Summer_Season = pd.concat([Extended_Summer_Season,Hot_Per[Hot_Per[Data_Col[9]]==i]]).sort_values(by=[Data_Col[0]], ascending=True)   \n",
    "    \n",
    "        elif (len(RightCheck) == 1):\n",
    "            Extended_Summer_Season = pd.concat([Extended_Summer_Season,Hot_Per[Hot_Per[Data_Col[9]]==i]]).sort_values(by=[Data_Col[0]], ascending=True)\n",
    "        \n",
    "    # removes the duplicates if there were heatwaves on any of the bounds\n",
    "    Extended_Summer_Season= Extended_Summer_Season.drop_duplicates(subset = [Data_Col[0]],keep='first')\n",
    "    #Clean up  dataset    \n",
    "    Extended_Summer_Season = Extended_Summer_Season.drop(['day','month','year'],axis=1)\n",
    "    \n",
    "    #fix the id's\n",
    "    #New id\n",
    "    Heatwaves = []\n",
    "    id_n = 0\n",
    "    for i in ids:\n",
    "        id_n = id_n+1\n",
    "        Event = Extended_Summer_Season[Extended_Summer_Season['id']==i]\n",
    "        Event['id'] = [id_n] * len(Event)\n",
    "        Heatwaves.append(Event)\n",
    "    Heatwaves = pd.concat(Heatwaves,axis=0)\n",
    "        \n",
    "    return(Heatwaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ced7fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Heatwave_Table_Generator(data):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    Data : DataFrame\n",
    "        The Heatwave dataframe\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    Heatwaves : DataFrames\n",
    "        An extension and clean up of the Heatwaves dataframe that provides more insight to the heatwaves.\n",
    "    \n",
    "    '''\n",
    "    # Add a new column called \"ave\" that calculates the average of the \"Max\" and \"Min\" columnsHeatwavesI\n",
    "    Heatwaves =data\n",
    "    #Get columns \n",
    "    \n",
    "    HW_Col = Heatwaves.columns\n",
    "    \n",
    "    \n",
    "    Heatwaves['Avg'] = (Heatwaves[HW_Col[1]] + Heatwaves[HW_Col[2]]) / 2\n",
    "\n",
    "    # Group the DataFrame by the \"id\" column and calculate the difference between the first and last dates of each group\n",
    "    duration = Heatwaves.groupby('id')['date'].agg([min, max]).reset_index()\n",
    "    print(duration)\n",
    "    duration['Duration'] = (pd.to_datetime(duration['max']) - pd.to_datetime(duration['min'])).dt.days + 1\n",
    "\n",
    "    # Merge the \"Duration\" column back into the original DataFrame\n",
    "    Heatwaves = pd.merge(Heatwaves, duration[['id', 'Duration']], on='id')\n",
    "\n",
    "    # Calculate the mean \"Max\", \"Min\", and \"ave\" values for each event\n",
    "    mean_values = Heatwaves.groupby('id')[[HW_Col[1], HW_Col[2], 'Avg']].mean().reset_index()\n",
    "\n",
    "    # Rename the columns to include \"Mean\" in the column names\n",
    "    mean_values = mean_values.rename(columns={HW_Col[1]: 'Max Mean', HW_Col[2]: 'Min Mean', 'Avg': 'Avg Mean'})\n",
    "\n",
    "    # Merge the \"Mean\" columns back into the original DataFrame\n",
    "    Heatwaves = pd.merge(Heatwaves, mean_values, on='id')\n",
    "\n",
    "    # Add a column for the total excess heat factor\n",
    "    Heatwaves['Total Excess Heat Factor'] = Heatwaves['Excess Heat FactorMax'] + Heatwaves['Excess Heat FactorMin']\n",
    "\n",
    "\n",
    "    # Define a function to calculate the intensity for a given heatwave event ID\n",
    "    def calculate_intensity(event_id):\n",
    "        event_data = Heatwaves[Heatwaves['id'] == event_id]\n",
    "        top_3_factors = event_data['Total Excess Heat Factor'].nlargest(3)\n",
    "        intensity = top_3_factors.mean()\n",
    "        return intensity\n",
    "\n",
    "    # Calculate the intensity for each heatwave event and add it to the Heatwaves DataFrame\n",
    "    Heatwaves['Intensity'] = Heatwaves['id'].apply(calculate_intensity)\n",
    "\n",
    "\n",
    "    # Round the columns to two decimal places\n",
    "    Heatwaves['Intensity'] = Heatwaves['Intensity'].round(2)\n",
    "    Heatwaves['Max Mean'] = Heatwaves['Max Mean'].round(2)\n",
    "    Heatwaves['Min Mean'] = Heatwaves['Min Mean'].round(2)\n",
    "    Heatwaves['Avg Mean'] = Heatwaves['Avg Mean'].round(2)\n",
    "    Heatwaves['Excess Heat FactorMax'] = Heatwaves['Excess Heat FactorMax'].round(2)\n",
    "    Heatwaves['Excess Heat FactorMin'] = Heatwaves['Excess Heat FactorMin'].round(2)\n",
    "    Heatwaves['Heat StressMax'] = Heatwaves['Heat StressMax'].round(2)\n",
    "    Heatwaves['Heat StressMin'] = Heatwaves['Heat StressMin'].round(2)\n",
    "    Heatwaves['Excess HeatMax'] = Heatwaves['Excess HeatMax'].round(2)\n",
    "    Heatwaves['Excess HeatMin'] = Heatwaves['Excess HeatMin'].round(2)\n",
    "    Heatwaves['Total Excess Heat Factor'] = Heatwaves['Total Excess Heat Factor'].round(2)\n",
    "    Heatwaves['Avg'] = Heatwaves['Avg'].round(2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Heatwaves_Data = Heatwaves.copy()\n",
    "\n",
    "    # create a function to assign the RHC category\n",
    "    def assign_rhc_category(intensity, duration):\n",
    "        if intensity < 10 and duration <= 4:\n",
    "            return 'RHC Cat 1'\n",
    "        elif intensity < 10 and duration > 4:\n",
    "            return 'RHC Cat 2'\n",
    "        elif intensity >= 10 and intensity < 20 and duration <= 4:\n",
    "            return 'RHC Cat 2'\n",
    "        elif intensity >= 10 and intensity < 20 and duration > 4:\n",
    "            return 'RHC Cat 3'\n",
    "        elif intensity >= 20 and intensity < 30 and duration <= 4:\n",
    "            return 'RHC Cat 3'\n",
    "        elif intensity >= 20 and intensity < 30 and duration > 4:\n",
    "            return 'RHC Cat 4'\n",
    "        elif intensity >= 30 and intensity < 40 and duration <= 4:\n",
    "            return 'RHC Cat 4'\n",
    "        elif intensity >= 30 and intensity < 40 and duration > 4:\n",
    "            return 'RHC Cat 5'\n",
    "        elif intensity >= 40 and intensity <= 50 and duration <= 4:\n",
    "            return 'RHC Cat 5'\n",
    "        elif intensity >= 40 and intensity <= 50 and duration > 4:\n",
    "            return 'RHC Cat 6'\n",
    "        elif intensity > 50 and duration <= 4:\n",
    "            return 'RHC Cat 6'\n",
    "        elif intensity > 40 and intensity <= 50 and duration > 4:\n",
    "            return 'RHC Cat 6'\n",
    "        else:\n",
    "            return 'RHC Cat 7'\n",
    "\n",
    "    # add the RHC column to the dataframe\n",
    "    Heatwaves['Rowe Heatwave Categorisation'] = Heatwaves.apply(lambda x: assign_rhc_category(x['Intensity'], x['Duration']), axis=1)\n",
    "    Heatwaves['Intensity'] = Heatwaves['Intensity'].astype(str) + ' \\u00b0C' + '\\xb2'  # Concatenate the string \"degC^2\"\n",
    "    Heatwaves['Max Mean'] = Heatwaves['Max Mean'].astype(str) + ' \\u00b0C'   # Concatenate the string \"degC\"\n",
    "    Heatwaves['Min Mean'] = Heatwaves['Min Mean'].astype(str) + ' \\u00b0C'   # Concatenate the string \"degC\"\n",
    "    Heatwaves['Avg Mean'] = Heatwaves['Avg Mean'].astype(str) + ' \\u00b0C'   # Concatenate the string \"degC\"\n",
    "    Heatwaves['Excess Heat FactorMax'] = Heatwaves['Excess Heat FactorMax'].astype(str) + ' \\u00b0C' + '\\xb2'    # Concatenate the string \"degC\"\n",
    "    Heatwaves['Excess Heat FactorMin'] = Heatwaves['Excess Heat FactorMin'].astype(str) + ' \\u00b0C' + '\\xb2'    # Concatenate the string \"degC\"\n",
    "    Heatwaves['Heat StressMax'] = Heatwaves['Heat StressMax'].astype(str) + ' \\u00b0C'\n",
    "    Heatwaves['Heat StressMin'] = Heatwaves['Heat StressMin'].astype(str) + ' \\u00b0C'\n",
    "    Heatwaves['Excess HeatMax'] = Heatwaves['Excess HeatMax'].astype(str) + ' \\u00b0C'\n",
    "    Heatwaves['Excess HeatMin'] = Heatwaves['Excess HeatMin'].astype(str) + ' \\u00b0C'\n",
    "    Heatwaves['Total Excess Heat Factor'] = Heatwaves['Total Excess Heat Factor'].astype(str) + ' \\u00b0C'\n",
    "    Heatwaves[HW_Col[1]] = Heatwaves[HW_Col[1]].astype(str) + ' \\u00b0C'   # Concatenate the string \"degC\"\n",
    "    Heatwaves[HW_Col[2]] = Heatwaves[HW_Col[2]].astype(str) + ' \\u00b0C'   # Concatenate the string \"degC\"\n",
    "    Heatwaves['Avg'] = Heatwaves['Avg'].astype(str) + ' \\u00b0C'   # Concatenate the string \"degC\"\n",
    "    Heatwaves['Duration'] = Heatwaves['Duration'].astype(str) + ' days'# Concatenate the string \"degC\"\n",
    "\n",
    "    # Rearrange columns in the Heatwaves dataframe\n",
    "    Heatwaves = Heatwaves.reindex(columns=['date', 'id', 'Rowe Heatwave Categorisation',HW_Col[1], HW_Col[2], 'Avg', 'Duration', 'Intensity', 'Max Mean', 'Min Mean', 'Avg Mean', 'Excess Heat FactorMax', 'Heat StressMax', 'Excess HeatMax', 'Excess Heat FactorMin', 'Heat StressMin', 'Excess HeatMin', 'Total Excess Heat Factor'])\n",
    "    # Define a list of colors to use\n",
    "    colors = ['white', 'gray']\n",
    "\n",
    "    # Create a dictionary to map each id to a color\n",
    "    id_color_map = {}\n",
    "    for i, id in enumerate(Heatwaves['id'].unique()):\n",
    "        id_color_map[id] = colors[i % len(colors)]\n",
    "\n",
    "    # Define a function to apply the color to each row based on the id\n",
    "    def apply_color(row):\n",
    "        color = id_color_map.get(row['id'])\n",
    "        return ['background-color: {}'.format(color)] * len(row)\n",
    "\n",
    "    # Apply the color to the dataframe\n",
    "    Heatwaves = Heatwaves.style.apply(apply_color, axis=1, subset=Heatwaves.columns)\n",
    "    return(Heatwaves,Heatwaves_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff015118",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea7ba2d6",
   "metadata": {},
   "source": [
    "## QUANTILE QUNATILE REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "940a3c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simple_QQ_Regression(Q_step, Historical, Present, Hist_Dates, Pres_Date):\n",
    "    '''\n",
    "    Q_step: Value\n",
    "    Must be non negative and at least less the 0.1\n",
    "    \n",
    "    Historical: DataFrame\n",
    "    Must have the date as the index\n",
    "    \n",
    "    Present: DataFrame\n",
    "    Must have the date as the index\n",
    "    \n",
    "    Hist_Dates/Pres_Dates: Vector\n",
    "    String of the dates in Y-M-D or the format that is given with the dataframes\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    number = Q_step\n",
    "    Historical_All = Historical\n",
    "    Present = Present\n",
    "    Hist_QQ_Dates_St = Hist_Dates[0]\n",
    "    Hist_QQ_Dates_En = Hist_Dates[1]\n",
    "\n",
    "    Pres_QQ_Date_St = Pres_Date[0]\n",
    "    Pres_QQ_Date_En = Pres_Date[1]\n",
    "    #^ call in using function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Historical_30 = Historical_All.loc[Hist_QQ_Dates_St:Hist_QQ_Dates_En].reset_index()\n",
    "    Present_30 = Present.loc[Pres_QQ_Date_St:Pres_QQ_Date_En].reset_index()\n",
    "\n",
    "\n",
    "\n",
    "    #Select a nparange value lemgth that goes from Q0 to Q1, and produce the quantiles\n",
    "    QPRE = Present_30.quantile(np.arange(0,1+number,number)).round(4)\n",
    "    QPRE = QPRE.rename_axis('Quantile').reset_index()\n",
    "\n",
    "    QHIS = Historical_30.quantile(np.arange(0,1+number,number)).round(4)\n",
    "    QHIS = QHIS.rename_axis('Quantile').reset_index()\n",
    "\n",
    "\n",
    "    Hist_All = Historical_All.reset_index()\n",
    "    #This above is the full range of historical data\n",
    "\n",
    "    #What we will do is append it to max and min values before combiniing with dat\n",
    "    Hist_Updated_Max = []\n",
    "    Hist_Updated_Min = []\n",
    "    Hist_Updated_Date = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #For loop for all dates \n",
    "    for i in range(0,len(Hist_All)):\n",
    "        Hist_Updated_Date.append(Hist_All['date'].loc[i])\n",
    "\n",
    "\n",
    "        #Now get all the information from the Q-Q data for max and min for each date\n",
    "        #MAX\n",
    "\n",
    "        #If data shows a nan value set the updated value to nan\n",
    "        if (math.isnan(Hist_All['tmax'].loc[i])== True):\n",
    "            Hist_Updated_Max.append(np.NaN)\n",
    "        else:\n",
    "            #Set Temp old \n",
    "            Temp_Old = Hist_All['tmax'].loc[i]\n",
    "\n",
    "\n",
    "\n",
    "            #So now we get the closest value for the max:\n",
    "            Column = ['tmax']\n",
    "\n",
    "            #This finds the value where the Q-Hist of the tmax is the minimum it can be for the tmax value presented\n",
    "            Min_val = np.abs(QHIS[Column] - Temp_Old).min()\n",
    "\n",
    "            #This finds the quantile*10^5 or by the decimla place you use to find the tmax\n",
    "            closest_index =  QHIS[np.abs(QHIS[Column]- Temp_Old) == Min_val].stack().idxmin()\n",
    "\n",
    "            #Now this will use the index to find the Present Vlaue to updayte the historical value to using the index/quantile*10^%\n",
    "            Hist_Updated_Max.append(QPRE[Column].loc[closest_index[0]].values[0])\n",
    "\n",
    "        #Now get all the information from the Q-Q data for max and min for each date\n",
    "        #MIN\n",
    "\n",
    "        #If data shows a nan value set the updated value to nan\n",
    "        if (math.isnan(Hist_All['tmin'].loc[i])== True):\n",
    "            Hist_Updated_Min.append(np.NaN)\n",
    "        else:\n",
    "            #Set Temp old \n",
    "            Temp_Old = Hist_All['tmin'].loc[i]\n",
    "\n",
    "\n",
    "            #So now we get the closest value for the max:\n",
    "            Column = ['tmin']\n",
    "\n",
    "            #This finds the value where the Q-Hist of the tmax is the minimum it can be for the tmax value presented\n",
    "            Min_val = np.abs(QHIS[Column] - Temp_Old).min()\n",
    "\n",
    "            #This finds the quantile*10^5 or by the decimla place you use to find the tmax\n",
    "            closest_index =  QHIS[np.abs(QHIS[Column]- Temp_Old) == Min_val].stack().idxmin()\n",
    "\n",
    "            #Now this will use the index to find the Present Vlaue to updayte the historical value to using the index/quantile*10^%\n",
    "            Hist_Updated_Min.append(QPRE[Column].loc[closest_index[0]].values[0])\n",
    "    Hist_Updated_Date = pd.DataFrame(Hist_Updated_Date, columns=['date'])\n",
    "    Hist_Updated_Max = pd.DataFrame(Hist_Updated_Max, columns=['tmax'])\n",
    "    Hist_Updated_Min = pd.DataFrame(Hist_Updated_Min, columns=['tmin'])\n",
    "\n",
    "\n",
    "    #Now combine altogether\n",
    "    Hist_Updated = pd.concat([Hist_Updated_Date, Hist_Updated_Max, Hist_Updated_Min], axis = 1)\n",
    "    return(Hist_Updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ecc89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65acd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c8b49a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d03ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1ed537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee15068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb617bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
